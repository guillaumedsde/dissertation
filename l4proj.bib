
@misc{2017SwampUPSessions,
  title = {2017 {{swampUP Sessions}} | {{Distroless Docker}}: {{Containerizing Apps}}, Not {{VMs}} - {{Matthew Moore}}},
  shorttitle = {2017 {{swampUP Sessions}} | {{Distroless Docker}}}
}
% == BibTeX quality report for 2017SwampUPSessions:
% Missing required field 'author'
% Missing required field 'howpublished'
% Missing required field 'year'

@article{alzhraniAutomatedBigText2016,
  title = {Automated {{Big Text Security Classification}}},
  author = {Alzhrani, Khudran and Rudd, Ethan M. and Boult, Terrance E. and Chow, C. Edward},
  year = {2016},
  month = oct,
  abstract = {In recent years, traditional cybersecurity safeguards have proven ineffective against insider threats. Famous cases of sensitive information leaks caused by insiders, including the WikiLeaks release of diplomatic cables and the Edward Snowden incident, have greatly harmed the U.S. government's relationship with other governments and with its own citizens. Data Leak Prevention (DLP) is a solution for detecting and preventing information leaks from within an organization's network. However, state-of-art DLP detection models are only able to detect very limited types of sensitive information, and research in the field has been hindered due to the lack of available sensitive texts. Many researchers have focused on document-based detection with artificially labeled ``confidential documents'' for which security labels are assigned to the entire document, when in reality only a portion of the document is sensitive. This type of whole-document based security labeling increases the chances of preventing authorized users from accessing non-sensitive information within sensitive documents. In this paper, we introduce Automated Classification Enabled by Security Similarity (ACESS), a new and innovative detection model that penetrates the complexity of big text security classification/detection. To analyze the ACESS system, we constructed a novel dataset, containing formerly classified paragraphs from diplomatic cables made public by the WikiLeaks organization. To our knowledge this paper is the first to analyze a dataset that contains actual formerly sensitive information annotated at paragraph granularity.},
  archivePrefix = {arXiv},
  eprint = {1610.06856},
  eprinttype = {arxiv},
  file = {/home/architect/Zotero/storage/RVEDGCXL/Alzhrani et al. - 2016 - Automated Big Text Security Classification.pdf},
  journal = {ArXiv161006856 Cs},
  keywords = {â›” No DOI found,No DOI found},
  language = {en},
  primaryClass = {cs}
}
% == BibTeX quality report for alzhraniAutomatedBigText2016:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Title looks like it was stored in title-case in Zotero

@incollection{apperleyBifocalDisplay,
  title = {Bifocal {{Display}}},
  booktitle = {The {{Encyclopedia}} of {{Human}}-{{Computer Interaction}}},
  author = {Apperley, Mark and Spence, Robert},
  edition = {2nd edition},
  abstract = {Ever wondered who invented the visualization technique behind the Apple Dock - used by millions of people every day? Bob Spence and Mark Apperley explain their invention of the Bifocal Display},
  file = {/home/architect/Zotero/storage/D6RP24GG/bifocal-display.html},
  language = {en}
}
% == BibTeX quality report for apperleyBifocalDisplay:
% Missing required field 'pages'
% Missing required field 'publisher'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{baeza-yatesVisualizationLargeAnswers1996,
  title = {Visualization of {{Large Answers}} in {{Text Databases}}},
  booktitle = {Proceedings of the {{Workshop}} on {{Advanced Visual Interfaces}}},
  author = {{Baeza-Yates}, Ricardo},
  year = {1996},
  pages = {101--107},
  publisher = {{ACM}},
  address = {{Gubbio, Italy}},
  doi = {10.1145/948449.948464},
  abstract = {Current user interfaces of full text retrieval systems do not help in the process of filtering the result of a query, usually very large. We address this problem and we propose a visual interface to handle the result of a query, based on a hybrid model for text. This graphical user interface provides several visual representations of the answer and its elements (queries, documents, and text), easing the analysis and the filtering process.},
  file = {/home/architect/Zotero/storage/KNNZRVHX/Baeza-Yates - 1996 - Visualization of Large Answers in Text Databases.pdf;/home/architect/Zotero/storage/AP9L3KP2/baeza-yates1996.html},
  isbn = {978-0-89791-834-3},
  series = {{{AVI}} '96}
}
% == BibTeX quality report for baeza-yatesVisualizationLargeAnswers1996:
% ? Title looks like it was stored in title-case in Zotero

@article{ballUnredactedUSEmbassy2011,
  title = {Unredacted {{US}} Embassy Cables Available Online after {{WikiLeaks}} Breach},
  author = {Ball, James},
  year = {2011},
  month = sep,
  issn = {0261-3077},
  abstract = {Guardian denies allegation in WikiLeaks statement that journalist disclosed passwords to archive},
  chapter = {US news},
  file = {/home/architect/Zotero/storage/XENY5EIG/unredacted-us-embassy-cables-online.html},
  journal = {The Guardian},
  language = {en-GB}
}
% == BibTeX quality report for ballUnredactedUSEmbassy2011:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'

@inproceedings{bartramContinuousZoomConstrained1995,
  title = {The Continuous Zoom: A Constrained Fisheye Technique for Viewing and Navigating Large Information Spaces},
  shorttitle = {The Continuous Zoom},
  booktitle = {Proceedings of the 8th Annual {{ACM}} Symposium on {{User}} Interface and Software Technology  - {{UIST}} '95},
  author = {Bartram, Lyn and Ho, Albert and Dill, John and Henigman, Frank},
  year = {1995},
  pages = {207--215},
  publisher = {{ACM Press}},
  address = {{Pittsburgh, Pennsylvania, United States}},
  doi = {10.1145/215585.215977},
  abstract = {Navigating and viewing large information spaces, such as hierarchically-organized networks from complex realtime systems, suffer the problems of viewing a large space on a small screen. Distorted-view approaches, such as fisheye techniques, have great potential to reduce these problems by representing detail within its larger context but introduce new issues of focus, transition between views and user disorientation from excessive distortion. We present a fisheyebased method which supports multiple focus points, enhances continuity through smooth transitions between views, and maintains location constraints to reduce the user's sense of spatial disorientation. These are important requirements for the representation and navigation of networked systems in supervisory control applications. The method consists of two steps: a global allocation of space to rectangular sections of the display, based on scale factors, followed by degree-of-interest adjustments. Previous versions of the algorithm relied solely on relative scale factors to assign size; we present a new version which allocates space more efficiently using a dynamically calculated degree of interest. In addition to the automatic system sizing, manual user control over the amount of space assigned each area is supported. The amount of detail shown in various parts of the network is controlled by pruning the hierarchy and presenting those sections in summary form.},
  file = {/home/architect/Zotero/storage/L4VTTZGJ/Bartram et al. - 1995 - The continuous zoom a constrained fisheye techniq.pdf;/home/architect/Zotero/storage/F36BLJXL/bartram1995.html},
  isbn = {978-0-89791-709-4},
  language = {en}
}
% == BibTeX quality report for bartramContinuousZoomConstrained1995:
% ? Unsure about the formatting of the booktitle

@inproceedings{baudischFishnetFisheyeWeb2004,
  title = {Fishnet, a {{Fisheye Web Browser}} with {{Search Term Popouts}}: {{A Comparative Evaluation}} with {{Overview}} and {{Linear View}}},
  shorttitle = {Fishnet, a {{Fisheye Web Browser}} with {{Search Term Popouts}}},
  booktitle = {Proceedings of the {{Working Conference}} on {{Advanced Visual Interfaces}}},
  author = {Baudisch, Patrick and Lee, Bongshin and Hanna, Libby},
  year = {2004},
  pages = {133--140},
  publisher = {{ACM}},
  address = {{Gallipoli, Italy}},
  doi = {10.1145/989863.989883},
  abstract = {Fishnet is a web browser that always displays web pages in their entirety, independent of their size. Fishnet accomplishes this by using a fisheye view, i.e. by showing a focus region at readable scale while spatially compressing page content above and below that region. Fishnet offers search term highlighting, and assures that those terms are readable by using "popouts". This allows users to visually scan search results within the entire page without scrolling.The scope of this paper is twofold. First, we present fishnet as a novel way of viewing the results of highlighted search and we discuss the design space. Second, we present a user study that helps practitioners determine which visualization technique--- fisheye view, overview, or regular linear view---to pick for which type of visual search scenario.},
  file = {/home/architect/Zotero/storage/QX3IZH39/Baudisch et al. - 2004 - Fishnet, a Fisheye Web Browser with Search Term Po.pdf;/home/architect/Zotero/storage/K3EK6YUA/baudisch2004.html},
  isbn = {978-1-58113-867-2},
  series = {{{AVI}} '04}
}
% == BibTeX quality report for baudischFishnetFisheyeWeb2004:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{berardiSemiAutomatedTextClassification2015,
  title = {Semi-{{Automated Text Classification}} for {{Sensitivity Identification}}},
  booktitle = {Proceedings of the 24th {{ACM International}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Berardi, Giacomo and Esuli, Andrea and Macdonald, Craig and Ounis, Iadh and Sebastiani, Fabrizio},
  year = {2015},
  month = oct,
  pages = {1711--1714},
  publisher = {{Association for Computing Machinery}},
  address = {{Melbourne, Australia}},
  doi = {10.1145/2806416.2806597},
  abstract = {Sensitive documents are those that cannot be made public, e.g., for personal or organizational privacy reasons. For instance, documents requested through Freedom of Information mechanisms must be manually reviewed for the presence of sensitive information before their actual release. Hence, tools that can assist human reviewers in spotting sensitive information are of great value to government organizations subject to Freedom of Information laws. We look at sensitivity identification in terms of semi-automated text classification (SATC), the task of ranking automatically classified documents so as to optimize the cost-effectiveness of human post-checking work. We use a recently proposed utility-theoretic approach to SATC that explicitly optimizes the chosen effectiveness function when ranking the documents by sensitivity; this is especially useful in our case, since sensitivity identification is a recall-oriented task, thus requiring the use of a recall-oriented evaluation measure such as F2. We show the validity of this approach by running experiments on a multi-label multi-class dataset of government documents manually annotated according to different types of sensitivity.},
  file = {/home/architect/Zotero/storage/PDPVZRC5/Berardi et al. - 2015 - Semi-Automated Text Classification for Sensitivity.pdf},
  isbn = {978-1-4503-3794-6},
  series = {{{CIKM}} '15}
}
% == BibTeX quality report for berardiSemiAutomatedTextClassification2015:
% ? Title looks like it was stored in title-case in Zotero

@book{binderReSearchingDigital2009,
  title = {({{Re}})Searching the Digital {{Bauhaus}}},
  editor = {Binder, Thomas and L{\"o}wgren, Jonas and Malmborg, Lone},
  year = {2009},
  publisher = {{Springer}},
  address = {{London}},
  file = {/home/architect/Zotero/storage/KBQSBJUI/Binder et al. - 2009 - (Re)searching the digital Bauhaus.pdf},
  isbn = {978-1-84800-349-1 978-1-84800-350-7},
  language = {en},
  lccn = {QA76.9.H85 R455 2009},
  note = {OCLC: ocn233933002},
  series = {Human-Computer Interaction Series}
}
% == BibTeX quality report for binderReSearchingDigital2009:
% Missing required field 'author'

@misc{bostockFisheyeDistortion2012,
  title = {Fisheye {{Distortion}}},
  author = {Bostock, Mike},
  year = {2012},
  file = {/home/architect/Zotero/storage/BYJVYDTQ/fisheye.html},
  howpublished = {https://bost.ocks.org/mike/fisheye/}
}
% == BibTeX quality report for bostockFisheyeDistortion2012:
% ? Title looks like it was stored in title-case in Zotero

@misc{bostockFisheyeGrid2019,
  title = {Fisheye {{Grid}}},
  author = {Bostock, Mike},
  year = {2019},
  abstract = {Mike Bostock's Block 2962761},
  file = {/home/architect/Zotero/storage/CRY47BDI/2962761.html},
  howpublished = {https://bl.ocks.org/mbostock/2962761}
}
% == BibTeX quality report for bostockFisheyeGrid2019:
% ? Title looks like it was stored in title-case in Zotero

@article{bucknerVisualizationTechniques,
  title = {Visualization {{Techniques}}},
  author = {Buckner, Nickie},
  pages = {6},
  abstract = {Many techniques have been described for viewing and exploring large information spaces. Often times these large data sets are represented in some hierarchical form. It is presumable that the same techniques that are effective for viewing general hierarchical structures would also be apt at displaying the Hierarchical Plans generated by many Artificial Intelligence Planning Systems. By surveying the current techniques available for visualizing general hierarchical information structures, hopefully a foundation will be laid for deriving equally effective techniques for the visualization of hierarchical plans and possibly plan search spaces.},
  file = {/home/architect/Zotero/storage/UTZLQEZD/Buckner - Visualization Techniques.pdf},
  keywords = {No DOI found},
  language = {en}
}
% == BibTeX quality report for bucknerVisualizationTechniques:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@misc{caplan-brickerChallengePreservingHistorical,
  title = {The {{Challenge}} of {{Preserving}} the {{Historical Record}} of \#{{MeToo}}},
  author = {{Caplan-Bricker}, Nora},
  abstract = {Evidence of the social-media movement should be collected, both because it matters and because it could disappear. But archivists face a battery of technical and ethical questions with few precedents.},
  file = {/home/architect/Zotero/storage/CG5LVMUB/the-challenge-of-preserving-the-historical-record-of-metoo.html},
  howpublished = {https://www.newyorker.com/tech/annals-of-technology/the-challenge-of-preserving-the-historical-record-of-metoo},
  journal = {The New Yorker},
  language = {en}
}
% == BibTeX quality report for caplan-brickerChallengePreservingHistorical:
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@article{changLIBSVMLibrarySupport2011,
  title = {{{LIBSVM}}: {{A}} Library for Support Vector Machines},
  shorttitle = {{{LIBSVM}}},
  author = {Chang, Chih-Chung and Lin, Chih-Jen},
  year = {2011},
  month = apr,
  volume = {2},
  pages = {1--27},
  issn = {21576904},
  doi = {10.1145/1961189.1961199},
  abstract = {LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems, theoretical convergence, multi-class classification, probability estimates, and parameter selection are discussed in detail.},
  file = {/home/architect/Zotero/storage/UIHDLH6C/Chang and Lin - 2011 - LIBSVM A library for support vector machines.pdf},
  journal = {ACM Trans. Intell. Syst. Technol.},
  language = {en},
  number = {3}
}
% == BibTeX quality report for changLIBSVMLibrarySupport2011:
% ? Possibly abbreviated journal title ACM Trans. Intell. Syst. Technol.

@patent{chowMethodApparatusFacilitating2013,
  title = {Method and Apparatus for Facilitating Document Sanitization},
  author = {Chow, Richard and Staddon, Jessica N. and Oberst, Ian S.},
  year = {2013},
  month = oct,
  assignee = {Palo Alto Research Center Inc},
  file = {/home/architect/Zotero/storage/SJBAASTU/Chow et al. - 2013 - Method and apparatus for facilitating document san.pdf},
  language = {en},
  nationality = {US},
  number = {US8566350B2}
}
% == BibTeX quality report for chowMethodApparatusFacilitating2013:
% I don't know how to quality-check patent references

@article{collinsDocuBurstVisualizingDocument2009,
  title = {{{DocuBurst}}: {{Visualizing Document Content}} Using {{Language Structure}}},
  shorttitle = {{{DocuBurst}}},
  author = {Collins, Christopher and Carpendale, Sheelagh and Penn, Gerald},
  year = {2009},
  volume = {28},
  pages = {1039--1046},
  issn = {1467-8659},
  doi = {10.1111/j.1467-8659.2009.01439.x},
  abstract = {Textual data is at the forefront of information management problems today. One response has been the development of visualizations of text data. These visualizations, commonly based on simple attributes such as relative word frequency, have become increasingly popular tools. We extend this direction, presenting the first visualization of document content which combines word frequency with the human-created structure in lexical databases to create a visualization that also reflects semantic content. DocuBurst is a radial, space-filling layout of hyponymy (the IS-A relation), overlaid with occurrence counts of words in a document of interest to provide visual summaries at varying levels of granularity. Interactive document analysis is supported with geometric and semantic zoom, selectable focus on individual words, and linked access to source text.},
  copyright = {\textcopyright{} 2009 The Author(s) Journal compilation \textcopyright{} 2009 The Eurographics Association and Blackwell Publishing Ltd.},
  file = {/home/architect/Zotero/storage/K32NYILX/Collins et al. - 2009 - DocuBurst Visualizing Document Content using Lang.pdf;/home/architect/Zotero/storage/P6GBVIGR/j.1467-8659.2009.01439.html},
  journal = {Comput. Graph. Forum},
  language = {en},
  number = {3}
}
% == BibTeX quality report for collinsDocuBurstVisualizingDocument2009:
% ? Possibly abbreviated journal title Comput. Graph. Forum

@article{cumbyMachineLearningBased,
  title = {A {{Machine Learning Based System}} for {{Semi}}-{{Automatically Redacting Documents}}},
  author = {Cumby, Chad and Ghani, Rayid},
  pages = {8},
  abstract = {Redacting text documents has traditionally been a mostly manual activity, making it expensive and prone to disclosure risks. This paper describes a semi-automated system to ensure a specified level of privacy in text data sets. Recent work has attempted to quantify the likelihood of privacy breaches for text data. We build on these notions to provide a means of obstructing such breaches by framing it as a multi-class classification problem. Our system gives users fine-grained control over the level of privacy needed to obstruct sensitive concepts present in that data. Additionally, our system is designed to respect a user-defined utility metric on the data (such as disclosure of a particular concept), which our methods try to maximize while anonymizing. We describe our redaction framework, algorithms, as well as a prototype tool built in to Microsoft Word that allows enterprise users to redact documents before sharing them internally and obscure client specific information. In addition we show experimental evaluation using publicly available data sets that show the effectiveness of our approach against both automated attackers and human subjects.The results show that we are able to preserve the utility of a text corpus while reducing disclosure risk of the sensitive concept.},
  file = {/home/architect/Zotero/storage/DERSJ2XQ/Cumby and Ghani - A Machine Learning Based System for Semi-Automatic.pdf},
  keywords = {No DOI found},
  language = {en}
}
% == BibTeX quality report for cumbyMachineLearningBased:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@incollection{de_rijke_towards_2014,
  title = {Towards a {{Classifier}} for {{Digital Sensitivity Review}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {McDonald, Graham and Macdonald, Craig and Ounis, Iadh and Gollins, Timothy},
  editor = {{de Rijke}, Maarten and Kenter, Tom and {de Vries}, Arjen P. and Zhai, ChengXiang and {de Jong}, Franciska and Radinsky, Kira and Hofmann, Katja},
  year = {2014},
  volume = {8416},
  pages = {500--506},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-06028-6_48},
  abstract = {The sensitivity review of government records is essential before they can be released to the official government archives, to prevent sensitive information (such as personal information, or that which is prejudicial to international relations) from being released. As records are typically reviewed and released after a period of decades, sensitivity review practices are still based on paper records. The transition to digital records brings new challenges, e.g. increased volume of digital records, making current practices impractical to use. In this paper, we describe our current work towards developing a sensitivity review classifier that can identify and prioritise potentially sensitive digital records for review. Using a test collection built from government records with real sensitivities identified by government assessors, we show that considering the entities present in each record can markedly improve upon a text classification baseline.},
  file = {/home/architect/Zotero/storage/DMFTFMRW/McDonald et al. - 2014 - Towards a Classifier for Digital Sensitivity Revie.pdf},
  isbn = {978-3-319-06027-9 978-3-319-06028-6},
  language = {en}
}
% == BibTeX quality report for de_rijke_towards_2014:
% ? Title looks like it was stored in title-case in Zotero

@article{diamantidisUnsupervisedStratificationCrossvalidation2000,
  title = {Unsupervised Stratification of Cross-Validation for Accuracy Estimation},
  author = {Diamantidis, N. A. and Karlis, D. and Giakoumakis, E. A.},
  year = {2000},
  month = jan,
  volume = {116},
  pages = {1--16},
  issn = {0004-3702},
  doi = {10.1016/s0004-3702(99)00094-6},
  abstract = {The rapid development of new learning algorithms increases the need for improved accuracy estimation methods. Moreover, methods allowing the comparison of several different learning algorithms are important for the performance evaluation of new ones. In this paper we propose new accuracy estimation methods which are extensions of the k-fold cross-validation method. The methods proposed construct cross-validation folds deterministically instead of using the random sampling approach. The deterministic construction of folds is performed using unsupervised stratification by exploiting the distribution of instances in the instance space. Our methods are based either on the one-center approach or on clustering procedures. These methods attempt to construct more representative folds, therefore reducing the bias of the resulting estimator. At the same time, our methods allow direct comparisons between the performance of learning algorithms in different experiments, since no randomness is present. A simulation experiment examining the performance of the proposed methods is reported, depicting their behavior in a variety of situations. The new methods reduce mainly the bias of the estimator.},
  file = {/home/architect/Zotero/storage/RDD247PJ/Diamantidis et al. - 2000 - Unsupervised stratification of cross-validation fo.pdf;/home/architect/Zotero/storage/ICSWZANM/S0004370299000946.html},
  journal = {Artificial Intelligence},
  language = {en},
  number = {1}
}

@article{el-assadyProgressiveLearningTopic2018,
  title = {Progressive {{Learning}} of {{Topic Modeling Parameters}}: {{A Visual Analytics Framework}}},
  shorttitle = {Progressive {{Learning}} of {{Topic Modeling Parameters}}},
  author = {{El-Assady}, Mennatallah and Sevastjanova, Rita and Sperrle, Fabian and Keim, Daniel and Collins, Christopher},
  year = {2018},
  month = jan,
  volume = {24},
  pages = {382--391},
  issn = {1077-2626},
  doi = {10.1109/tvcg.2017.2745080},
  abstract = {Topic modeling algorithms are widely used to analyze the thematic composition of text corpora but remain difficult to interpret and adjust. Addressing these limitations, we present a modular visual analytics framework, tackling the understandability and adaptability of topic models through a user-driven reinforcement learning process which does not require a deep understanding of the underlying topic modeling algorithms. Given a document corpus, our approach initializes two algorithm configurations based on a parameter space analysis that enhances document separability. We abstract the model complexity in an interactive visual workspace for exploring the automatic matching results of two models, investigating topic summaries, analyzing parameter distributions, and reviewing documents. The main contribution of our work is an iterative decision-making technique in which users provide a document-based relevance feedback that allows the framework to converge to a user-endorsed topic distribution. We also report feedback from a two-stage study which shows that our technique results in topic model quality improvements on two independent measures.},
  file = {/home/architect/Zotero/storage/6V33HR6N/El-Assady et al. - 2018 - Progressive Learning of Topic Modeling Parameters.pdf;/home/architect/Zotero/storage/X7WZ9QES/el-assady2017.html},
  journal = {IEEE Trans. Visual. Comput. Graphics},
  language = {en},
  number = {1}
}
% == BibTeX quality report for el-assadyProgressiveLearningTopic2018:
% ? Possibly abbreviated journal title IEEE Trans. Visual. Comput. Graphics
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{el-assadyVisArgueVisualText2016,
  title = {{{VisArgue}} : {{A Visual Text Analytics Framework}} for the {{Study}} of {{Deliberative Communication}}},
  shorttitle = {{{VisArgue}}},
  booktitle = {{{PolText}} 2016 - {{The International Conference}} on the {{Advancesin Computational Analysis}} of {{Political Text}}},
  author = {{El-Assady}, Mennatallah and Gold, Valentin and {Hautli-Janisz}, Annette and Jentner, Wolfgang and Butt, Miriam and Holzinger, Katharina and Keim, Daniel A.},
  year = {2016},
  pages = {31--36},
  abstract = {For the last two decades, deliberative democracy has been intensively debated within political science and other related fields. Only recently, deliberation research has experienced a computational turn. In this paper, we present a linguistic and visual framework for the study of deliberative communication. The framework includes a range of visual analytics approaches to support research into deliberation. In particular, we propose a range of visualizations for highlighting deliberative patterns over time, speakers, and debates.},
  copyright = {terms-of-use},
  file = {/home/architect/Zotero/storage/2TGZFDSV/El-Assady et al. - 2016 - VisArgue  A Visual Text Analytics Framework for t.pdf;/home/architect/Zotero/storage/56CFQGHI/37650.html},
  isbn = {978-953-6457-92-2},
  language = {eng}
}
% == BibTeX quality report for el-assadyVisArgueVisualText2016:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@book{EncyclopediaHumanComputerInteraction,
  title = {The {{Encyclopedia}} of {{Human}}-{{Computer Interaction}}, 2nd {{Ed}}.},
  abstract = {The Encyclopedia of Human-Computer Interaction, 2nd Ed.. Free textbooks written by more than 100 leading designers, bestselling authors, and Ivy League professors. We have assembled our textbooks in a gigantic encyclopedia, whose 4,000+ pages cover the design of interactive products and services such as websites, household objects, smartphones, computer s...},
  file = {/home/architect/Zotero/storage/N8VYB2GS/the-encyclopedia-of-human-computer-interaction-2nd-ed.html},
  language = {en}
}
% == BibTeX quality report for EncyclopediaHumanComputerInteraction:
% Missing required field 'author'
% Missing required field 'publisher'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{goldExploratoryTextAnalysis2015,
  title = {Exploratory {{Text Analysis}} Using {{Lexical Episode Plots}}},
  booktitle = {{{EuroVis}} : {{The EG}}/{{VGTC Conference}} on {{Visualization}}},
  author = {Gold, Valentin and Rohrdantz, Christian and {El-Assady}, Mennatallah},
  year = {2015},
  pages = {85--90},
  doi = {10.2312/eurovisshort.20151130},
  abstract = {In this paper, we present Lexical Episode Plots, a novel automated text-mining and visual analytics approach for exploratory text analysis. In particular, we first describe an algorithm for automatically annotating text regions to examine prominent themes within natural language texts. The algorithm is based on lexical chaining to find spans of text in which the frequency of a term is significantly higher than its average in the document. In a second step we present an interactive visualization supporting the exploration and interpretation of Lexical Episodes. The visualization links higher-level thematic structures with content-level details. The methodological capabilities of our approach are illustrated by analyzing the televised US presidential election debates.},
  file = {/home/architect/Zotero/storage/Y5GBB76G/Gold et al. - 2015 - Exploratory Text Analysis using Lexical Episode Pl.pdf;/home/architect/Zotero/storage/LF79AM99/32053.html},
  language = {eng}
}
% == BibTeX quality report for goldExploratoryTextAnalysis2015:
% Missing required field 'publisher'

@article{gollinsUsingInformationRetrieval,
  title = {On {{Using Information Retrieval}} for the {{Selection}} and {{Sensitivity Review}} of {{Digital Public Records}}},
  author = {Gollins, Timothy and McDonald, Graham and Macdonald, Craig and Ounis, Iadh},
  pages = {2},
  file = {/home/architect/Zotero/storage/LCYTYGLW/Gollins et al. - On Using Information Retrieval for the Selection a.pdf;/home/architect/Zotero/storage/U9XJB7M3/Gollins et al. - On Using Information Retrieval for the Selection a.pdf},
  keywords = {â›” No DOI found,No DOI found},
  language = {en}
}
% == BibTeX quality report for gollinsUsingInformationRetrieval:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@misc{GoogleContainerToolsDistroless2020,
  title = {{{GoogleContainerTools}}/Distroless},
  year = {2020},
  month = feb,
  abstract = {ðŸ¥‘  Language focused docker images, minus the operating system.},
  copyright = {Apache-2.0},
  howpublished = {GoogleContainerTools}
}
% == BibTeX quality report for GoogleContainerToolsDistroless2020:
% Missing required field 'author'
% ? Title looks like it was stored in lower-case in Zotero

@inproceedings{greenbergFisheyeTextEditor1996,
  ids = {greenbergFisheyeTextEditor1996a},
  title = {A Fisheye Text Editor for Relaxed-{{WYSIWIS}} Groupware},
  booktitle = {Conference Companion on {{Human}} Factors in Computing Systems Common Ground - {{CHI}} '96},
  author = {Greenberg, Saul},
  year = {1996},
  pages = {212--213},
  publisher = {{ACM Press}},
  address = {{Vancouver, British Columbia, Canada}},
  doi = {10.1145/257089.257285},
  abstract = {Participants in a real-time groupware conference require a sense of awareness about other people's interactions within a large shared workspace. Fisheye views can afford this awareness by assigning a focal point to each participant. The fisheye effect around these multiple focal points provides peripheral awareness by showing people's location in the global context, and by magnifying the area around their work to highlight interaction details. An adjustable magnification function lets people customize the awareness information to fit their collaboration needs. A fisheye text editor illustrates how this can be accomplished.},
  file = {/home/architect/Zotero/storage/39EDATHR/Greenberg - 1996 - A fisheye text editor for relaxed-WYSIWIS groupwar.pdf;/home/architect/Zotero/storage/4C7ZA2BP/Greenberg - 1996 - A fisheye text editor for relaxed-WYSIWIS groupwar.pdf;/home/architect/Zotero/storage/3FPWJZES/greenberg1996.html;/home/architect/Zotero/storage/7ZBIXTWI/greenberg1996.html},
  isbn = {978-0-89791-832-9},
  language = {en}
}

@misc{GTO703ShapleyValue,
  title = {{{GTO}}-7-03: {{The Shapley Value}}},
  shorttitle = {{{GTO}}-7-03},
  abstract = {This video from Game Theory Online (http://www.game-theory-class.org) defines the Shapley Value, a prominent way of dividing profits within a coalition based on a formal notion of marginal bargaining power.  It features Matt Jackson (Stanford).},
  keywords = {\#nosource}
}
% == BibTeX quality report for GTO703ShapleyValue:
% Missing required field 'author'
% Missing required field 'howpublished'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@book{hastieElementsStatisticalLearning2013,
  title = {The {{Elements}} of {{Statistical Learning}}: {{Data Mining}}, {{Inference}}, and {{Prediction}}},
  shorttitle = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2013},
  month = nov,
  publisher = {{Springer Science \& Business Media}},
  abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
  file = {/home/architect/Zotero/storage/YCD4HAJF/Hastie et al. - 2013 - The Elements of Statistical Learning Data Mining,.pdf},
  googlebooks = {yPfZBwAAQBAJ},
  isbn = {978-0-387-21606-5},
  language = {en}
}
% == BibTeX quality report for hastieElementsStatisticalLearning2013:
% ? Title looks like it was stored in title-case in Zotero

@techreport{informationcommissionersofficePersonalInformationSection,
  title = {Personal Information (Section 40 and Regulation 13): {{Freedom}} of {{Information Act Environmental}} Information {{Regulations}}},
  shorttitle = {Personal Information},
  author = {{Information Commissioner's Office}},
  pages = {48},
  file = {/home/architect/Zotero/storage/Q3N44HTZ/Personal_information_(section_40_and_regulation_13.pdf},
  language = {en},
  number = {20190925 v2.3}
}
% == BibTeX quality report for informationcommissionersofficePersonalInformationSection:
% Missing required field 'institution'
% Missing required field 'year'

@misc{InterpretableMachineLearning,
  title = {Interpretable {{Machine Learning Using LIME Framework}} - {{Kasia Kulma}} ({{PhD}}), {{Data Scientist}}, {{Aviva}}},
  abstract = {This presentation was filmed at the London Artificial Intelligence \&amp; Deep Learning Meetup: https://www.meetup.com/London-Artific....

Enjoy the slides: https://www.slideshare.net/0xdata/int.... 

- - - 

Kasia discussed complexities of interpreting black-box algorithms and how these may affect some industries. She presented the most popular methods of interpreting Machine Learning classifiers, for example, feature importance or partial dependence plots and Bayesian networks. Finally, she introduced Local Interpretable Model-Agnostic Explanations (LIME) framework for explaining predictions of black-box learners \textendash{} including text- and image-based models - using breast cancer data as a specific case scenario.

Kasia Kulma is a Data Scientist at Aviva with a soft spot for R. She obtained a PhD (Uppsala University, Sweden) in evolutionary biology in 2013 and has been working on all things data ever since. For example, she has built recommender systems, customer segmentations, predictive models and now she is leading an NLP project at the UK's leading insurer. In spare time she tries to relax by hiking \&amp; camping, but if that doesn't work ;) she co-organizes R-Ladies meetups and writes a data science blog R-tastic (https://kkulma.github.io/).

https://www.linkedin.com/in/kasia-kul...},
  keywords = {\#nosource}
}
% == BibTeX quality report for InterpretableMachineLearning:
% Missing required field 'author'
% Missing required field 'howpublished'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@misc{ItAlwaysSeems,
  title = {``{{It}} Always Seems Impossible until It Is Done'': {{Sensitivity Review}} and {{Digital Records}}'},
  shorttitle = {``{{It}} Always Seems Impossible until It Is Done''},
  abstract = {Joint seminer: Archives \&amp; Society and Digital History seminar
Speaker: Anthea Seles , The National Archives
http://ihrdighist.blogs.sas.ac.uk/201... 

Abstract

``It always seems impossible until it is done'': Sensitivity Review and Digital Records' touches on the experiences and lessons learned at The National Archives on digital records sensitivity in order to enable the scalable and repeatable transfer of digital records, and user access, while protecting personal data and sensitive information from release. The presentation draws directly from the published research report The application of technology- assisted review to born-digital records transfer, Inquiries and beyond (February 2016) and will explore the software testing The National Archives carried out to try and develop a scalable approach to the appraisal and selection, and sensitivity review of large unstructured digital records collections. During the talk Dr. Seles will also endeavour to touch the impact these approaches have on the historical record and the strengths and weaknesses data analytic software can have on future digital historical research.

Bio

Dr. Anthea Seles is graduate of Masters of Archival Studies (MAS) programme at the University of British Columbia (Vancouver, British Columbia) and a recent doctoral graduate from the Department of Information Studies at the University College London. Her doctorate: The Transferability of Trusted Digital Repository Standards to an East African Context was awarded the 2016 Digital Preservation Coalition Award for Most Distinguished Student Work in Digital Preservation. Dr. Seles has worked in the field of archives and records management for over 10 years, during that time she worked for the International Records Management Trust. At the Trust she examined topics such as data integrity for open data, accountability and transparency and worked with organisations like the African Union Commission (Addis Ababa) and the International Criminal Courts (The Hague, Netherlands). She joined The National Archives in 2014 as the Digital Records and Transfer Manager. In her position she and her team oversee the transfer of government digital records transfers. As part of this process she advises on the capabilities of data analytics/eDiscovery software for the purposes of digital appraisal and selection and sensitivity review.

Dr. Seles has lectured and spoken extensively on digital preservation topics at international conferences, symposia and universities. Most recently she has presented on the topics of digital sensitivity review at the University of Glasgow (November 2016), and on the impact of digital records on historical research at King's College (December 2016). She has also written several papers on digital record-keeping in Africa, based on her experiences at the International Records Management Trust.}
}
% == BibTeX quality report for ItAlwaysSeems:
% Missing required field 'author'
% Missing required field 'howpublished'
% Missing required field 'year'

@incollection{jose_enhancing_2017,
  title = {Enhancing {{Sensitivity Classification}} with {{Semantic Features Using Word Embeddings}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {McDonald, Graham and Macdonald, Craig and Ounis, Iadh},
  editor = {Jose, Joemon M and Hauff, Claudia and Alt\i{}ngovde, Ismail Sengor and Song, Dawei and Albakour, Dyaa and Watt, Stuart and Tait, John},
  year = {2017},
  volume = {10193},
  pages = {450--463},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-56608-5_35},
  abstract = {Government documents must be reviewed to identify any sensitive information they may contain, before they can be released to the public. However, traditional paper-based sensitivity review processes are not practical for reviewing born-digital documents. Therefore, there is a timely need for automatic sensitivity classification techniques, to assist the digital sensitivity review process. However, sensitivity is typically a product of the relations between combinations of terms, such as who said what about whom, therefore, automatic sensitivity classification is a difficult task. Vector representations of terms, such as word embeddings, have been shown to be effective at encoding latent term features that preserve semantic relations between terms, which can also be beneficial to sensitivity classification. In this work, we present a thorough evaluation of the effectiveness of semantic word embedding features, along with term and grammatical features, for sensitivity classification. On a test collection of government documents containing real sensitivities, we show that extending text classification with semantic features and additional term n-grams results in significant improvements in classification effectiveness, correctly classifying 9.99\% more sensitive documents compared to the text classification baseline.},
  file = {/home/architect/Zotero/storage/F268VJKN/McDonald et al. - 2017 - Enhancing Sensitivity Classification with Semantic.pdf;/home/architect/Zotero/storage/MANDGDSJ/McDonald et al. - 2017 - Enhancing Sensitivity Classification with Semantic.pdf;/home/architect/Zotero/storage/MJEB7FER/McDonald et al. - 2017 - Enhancing Sensitivity Classification with Semantic.pdf;/home/architect/Zotero/storage/TBXGTEBF/McDonald et al. - 2017 - Enhancing Sensitivity Classification with Semantic.pdf;/home/architect/Zotero/storage/V4LF6YP3/McDonald et al. - 2017 - Enhancing Sensitivity Classification with Semantic.pdf},
  isbn = {978-3-319-56607-8 978-3-319-56608-5},
  language = {en}
}
% == BibTeX quality report for jose_enhancing_2017:
% ? Title looks like it was stored in title-case in Zotero

@article{kesslerScattertextBrowserBasedTool2017,
  title = {Scattertext: A {{Browser}}-{{Based Tool}} for {{Visualizing}} How {{Corpora Differ}}},
  shorttitle = {Scattertext},
  author = {Kessler, Jason S.},
  year = {2017},
  month = apr,
  abstract = {Scattertext is an open source tool for visualizing linguistic variation between document categories in a language-independent way. The tool presents a scatterplot, where each axis corresponds to the rank-frequency a term occurs in a category of documents. Through a tie-breaking strategy, the tool is able to display thousands of visible term-representing points and find space to legibly label hundreds of them. Scattertext also lends itself to a query-based visualization of how the use of terms with similar embeddings differs between document categories, as well as a visualization for comparing the importance scores of bag-of-words features to univariate metrics.},
  archivePrefix = {arXiv},
  eprint = {1703.00565},
  eprinttype = {arxiv},
  file = {/home/architect/Zotero/storage/ZZIF9CD7/Kessler - 2017 - Scattertext a Browser-Based Tool for Visualizing .pdf;/home/architect/Zotero/storage/VJLK7ICL/1703.html},
  journal = {ArXiv170300565 Cs},
  keywords = {No DOI found},
  primaryClass = {cs}
}
% == BibTeX quality report for kesslerScattertextBrowserBasedTool2017:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'

@article{lemaitreImbalancedlearnPythonToolbox2017,
  title = {Imbalanced-Learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning},
  shorttitle = {Imbalanced-Learn},
  author = {Lema{\^i}tre, Guillaume and Nogueira, Fernando and Aridas, Christos K.},
  year = {2017},
  month = jan,
  volume = {18},
  pages = {559--563},
  issn = {1532-4435},
  abstract = {imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the-art methods can be categorized into 4 groups: (i) under-sampling, (ii) over-sampling, (iii) combination of over-and under-sampling, and (iv) ensemble learning methods. The proposed toolbox depends only on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. Source code, binaries, and documentation can be downloaded from https://github.com/scikit-learn-contrib/imbalanced-learn.},
  file = {/home/architect/Zotero/storage/I9VV6N9U/LemaÃ®tre et al. - 2017 - Imbalanced-learn a python toolbox to tackle the c.pdf},
  journal = {J. Mach. Learn. Res.},
  keywords = {No DOI found},
  number = {1}
}
% == BibTeX quality report for lemaitreImbalancedlearnPythonToolbox2017:
% ? Possibly abbreviated journal title J. Mach. Learn. Res.

@patent{libinAutomaticProtectionPartial2019,
  title = {Automatic Protection of Partial Document Content},
  author = {Libin, Phil},
  year = {2019},
  month = aug,
  abstract = {Protecting a fragment of a document includes automatically detecting the fragment without user intervention based on the content of the fragment and/or the context of the fragment within a set of documents, selectively encrypting the fragment to prevent unauthorized access, and providing an alternative view of the fragment that prevents viewing and access of content corresponding to the fragment unless a decryption password is provided. Automatically detecting the fragment may include detecting numbers and alphanumeric sequences of sufficient length that do not represent commonly known abbreviations, detecting generic terms, detecting proper names, detecting terms signifying a type of content, detecting mutual location of terms and sensitive content, and/or detecting user defined terms. The generic terms may correspond to password, passcode, credentials, user name, account, ID, login, confidential, and/or sensitive. The proper names may be names of financial organizations and security organizations.},
  assignee = {Evernote Corp},
  file = {/home/architect/Zotero/storage/MGPR2HT4/Libin - 2019 - Automatic protection of partial document content.pdf},
  nationality = {US},
  number = {US20190243983A1}
}
% == BibTeX quality report for libinAutomaticProtectionPartial2019:
% I don't know how to quality-check patent references

@article{lundbergUnifiedApproachInterpreting2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  author = {Lundberg, Scott and Lee, Su-In},
  year = {2017},
  month = nov,
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  archivePrefix = {arXiv},
  eprint = {1705.07874},
  eprinttype = {arxiv},
  file = {/home/architect/Zotero/storage/ESJI93Z6/Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf},
  journal = {ArXiv170507874 Cs Stat},
  keywords = {No DOI found},
  language = {en},
  primaryClass = {cs, stat}
}
% == BibTeX quality report for lundbergUnifiedApproachInterpreting2017:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Title looks like it was stored in title-case in Zotero

@incollection{lundbergUnifiedApproachInterpreting2017a,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Lundberg, Scott M and Lee, Su-In},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {4765--4774},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/architect/Zotero/storage/YX933PYC/7062-a-unified-approach-to-interpreting-model-predicti.html}
}
% == BibTeX quality report for lundbergUnifiedApproachInterpreting2017a:
% ? Title looks like it was stored in title-case in Zotero

@article{martineauDeltaTFIDFImproved,
  title = {Delta {{TFIDF}}: {{An Improved Feature Space}} for {{Sentiment Analysis}}},
  author = {Martineau, Justin and Finin, Tim},
  pages = {4},
  abstract = {Mining opinions and sentiment from social networking sites is a popular application for social media systems. Common approaches use a machine learning system with a bag of words feature set. We present Delta TFIDF, an intuitive general purpose technique to efficiently weight word scores before classification. Delta TFIDF is easy to compute, implement, and understand. We use Support Vector Machines to show that Delta TFIDF significantly improves accuracy for sentiment analysis problems using three well known data sets.},
  file = {/home/architect/Zotero/storage/UAJTUXMP/Martineau and Finin - Delta TFIDF An Improved Feature Space for Sentime.pdf},
  keywords = {No DOI found},
  language = {en}
}
% == BibTeX quality report for martineauDeltaTFIDFImproved:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@article{mcdonaldFrameworkEnhancedText2015,
  title = {A {{Framework}} for {{Enhanced Text Classification}} in {{Sensitivity}} and {{Reputation Management}}},
  author = {McDonald, Graham S.},
  year = {2015},
  month = sep,
  doi = {10.14236/ewic/fdia2015.15},
  file = {/home/architect/Zotero/storage/CHWYCCCJ/McDonald - 2015 - A Framework for Enhanced Text Classification in Se.pdf;/home/architect/Zotero/storage/KMIKY9BM/hosted-document.html}
}
% == BibTeX quality report for mcdonaldFrameworkEnhancedText2015:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{mcdonaldHowSensitivityClassification2019,
  ids = {mcdonaldHowSensitivityClassification2019a},
  title = {How {{Sensitivity Classification Effectiveness Impacts Reviewers}} in {{Technology}}-{{Assisted Sensitivity Review}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Human Information Interaction}} and {{Retrieval}}  - {{CHIIR}} '19},
  author = {McDonald, Graham and Macdonald, Craig and Ounis, Iadh},
  year = {2019},
  pages = {337--341},
  publisher = {{ACM Press}},
  address = {{Glasgow, Scotland UK}},
  doi = {10.1145/3295750.3298962},
  abstract = {All government documents that are released to the public must first be manually reviewed to identify and protect any sensitive information, e.g. confidential information. However, the unassisted manual sensitivity review of born-digital documents is not practical due to, for example, the volume of documents that are created. Previous work has shown that sensitivity classification can be effective for predicting if a document contains sensitive information. However, since all of the released documents must be manually reviewed, it is important to know if sensitivity classification can assist sensitivity reviewers in making their sensitivity judgements. Hence, in this paper, we conduct a digital sensitivity review user study, to investigate if the accuracy of sensitivity classification effects the number of documents that a reviewer correctly judges to be sensitive or not (reviewer accuracy) and the time that it takes to sensitivity review a document (reviewing speed). Our results show that providing reviewers with sensitivity classification predictions, from a classifier that achieves 0.7 Balanced Accuracy, results in a 38\% increase in mean reviewer accuracy and an increase of 72\% in mean reviewing speeds, compared to when reviewers are not provided with predictions. Overall, our findings demonstrate that sensitivity classification is a viable technology for assisting with the sensitivity review of born-digital government documents.},
  file = {/home/architect/Zotero/storage/7X4DPQMW/McDonald et al. - 2019 - How Sensitivity Classification Effectiveness Impac.pdf;/home/architect/Zotero/storage/8EQ6IW2A/McDonald et al. - 2019 - How Sensitivity Classification Effectiveness Impac.pdf;/home/architect/Zotero/storage/A6NAM552/McDonald et al. - 2019 - How Sensitivity Classification Effectiveness Impac.pdf},
  isbn = {978-1-4503-6025-8},
  language = {en}
}
% == BibTeX quality report for mcdonaldHowSensitivityClassification2019:
% ? Title looks like it was stored in title-case in Zotero

@incollection{mcdonaldMaximisingOpennessDigital2018,
  title = {Towards {{Maximising Openness}} in {{Digital Sensitivity Review Using Reviewing Time Predictions}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {McDonald, Graham and Macdonald, Craig and Ounis, Iadh},
  editor = {Pasi, Gabriella and Piwowarski, Benjamin and Azzopardi, Leif and Hanbury, Allan},
  year = {2018},
  volume = {10772},
  pages = {699--706},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-76941-7_65},
  abstract = {The adoption of born-digital documents, such as email, by governments, such as in the UK and USA, has resulted in a large backlog of born-digital documents that must be sensitivity reviewed before they can be opened to the public, to ensure that no sensitive information is released, e.g. personal or confidential information. However, it is not practical to review all of the backlog with the available reviewing resources and, therefore, there is a need for automatic techniques to increase the number of documents that can be opened within a fixed reviewing time budget. In this paper, we conduct a user study and use the log data to build models to predict reviewing times for an average sensitivity reviewer. Moreover, we show that using our reviewing time predictions to select the order that documents are reviewed can markedly increase the ratio of reviewed documents that are released to the public, e.g. +30\% for collections with high levels of sensitivity, compared to reviewing by shortest document first. This, in turn, increases the total number of documents that are opened to the public within a fixed reviewing time budget, e.g. an extra 200 documents in 100 hours reviewing.},
  file = {/home/architect/Zotero/storage/XZBNW7KI/McDonald et al. - 2018 - Towards Maximising Openness in Digital Sensitivity.pdf},
  isbn = {978-3-319-76940-0 978-3-319-76941-7},
  language = {en}
}
% == BibTeX quality report for mcdonaldMaximisingOpennessDigital2018:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{mcdonaldStudySVMKernel2017,
  title = {A {{Study}} of {{SVM Kernel Functions}} for {{Sensitivity Classification Ensembles}} with {{POS Sequences}}},
  booktitle = {Proceedings of the 40th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}  - {{SIGIR}} '17},
  author = {McDonald, Graham and {Garc{\'i}a-Pedrajas}, Nicol{\'a}s and Macdonald, Craig and Ounis, Iadh},
  year = {2017},
  pages = {1097--1100},
  publisher = {{ACM Press}},
  address = {{Shinjuku, Tokyo, Japan}},
  doi = {10.1145/3077136.3080731},
  abstract = {Freedom of Information (FOI) laws legislate that government documents should be opened to the public. However, many government documents contain sensitive information, such as con dential information, that is exempt from release. erefore, government documents must be sensitivity reviewed prior to release, to identify and close any sensitive information. With the adoption of born-digital documents, such as email, there is a need for automatic sensitivity classi cation to assist digital sensitivity review. SVM classi ers and Part-of-Speech sequences have separately been shown to be promising for sensitivity classi cation. However, sequence classi cation methodologies, and speci cally SVM kernel functions, have not been fully investigated for sensitivity classi cation. erefore, in this work, we present an evaluation of ve SVM kernel functions for sensitivity classi cation using POS sequences. Moreover, we show that an ensemble classi er that combines POS sequence classi cation with text classi cation can signi cantly improve sensitivity classi cation e ectiveness (+6.09\% F2) compared with a text classi cation baseline, according to McNemar's test of signi cance.},
  file = {/home/architect/Zotero/storage/Z6PEEIW4/McDonald et al. - 2017 - A Study of SVM Kernel Functions for Sensitivity Cl.pdf},
  isbn = {978-1-4503-5022-8},
  language = {en}
}
% == BibTeX quality report for mcdonaldStudySVMKernel2017:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{mcdonaldUsingPartofSpeechNgrams2015,
  title = {Using {{Part}}-of-{{Speech N}}-Grams for {{Sensitive}}-{{Text Classification}}},
  booktitle = {Proceedings of the 2015 {{International Conference}} on {{Theory}} of {{Information Retrieval}} - {{ICTIR}} '15},
  author = {McDonald, Graham and Macdonald, Craig and Ounis, Iadh},
  year = {2015},
  pages = {381--384},
  publisher = {{ACM Press}},
  address = {{Northampton, Massachusetts, USA}},
  doi = {10.1145/2808194.2809496},
  abstract = {Freedom of Information legislations in many western democracies, including the United Kingdom (UK) and the United States of America (USA), state that citizens have typically the right to access government documents. However, certain sensitive information is exempt from release into the public domain. For example, in the UK, FOIA Exemption 27 (International Relations) excludes the release of Information that might damage the interests of the UK abroad. Therefore, the process of reviewing government documents for sensitivity is essential to determine if a document must be redacted before it is archived, or closed until the information is no longer sensitive. With the increased volume of digital government documents in recent years, there is a need for new tools to assist the digital sensitivity review process. Therefore, in this paper we propose an automatic approach for identifying sensitive text in documents by measuring the amount of sensitivity in sequences of text. Using government documents reviewed by trained sensitivity reviewers, we focus on an aspect of FOIA Exemption 27 which can have a major impact on international relations, namely information supplied in confidence. We show that our approach leads to markedly increased recall of sensitive text, while achieving a very high level of precision, when compared to a baseline that has been shown to be effective at identifying sensitive text in other domains.},
  file = {/home/architect/Zotero/storage/44RDC47A/McDonald et al. - 2015 - Using Part-of-Speech N-grams for Sensitive-Text Cl.pdf;/home/architect/Zotero/storage/5FVAS2SY/McDonald et al. - 2015 - Using Part-of-Speech N-grams for Sensitive-Text Cl.pdf;/home/architect/Zotero/storage/7G8Q8PPS/McDonald et al. - 2015 - Using Part-of-Speech N-grams for Sensitive-Text Cl.pdf},
  isbn = {978-1-4503-3833-2},
  language = {en}
}

@article{mooreBloodTransfusionIndependent1997,
  title = {Blood {{Transfusion}}: {{An Independent Risk Factor}} for {{Postinjury Multiple Organ Failure}}},
  shorttitle = {Blood {{Transfusion}}},
  author = {Moore, Frederick A. and Moore, Ernest E. and Sauaia, Angela},
  year = {1997},
  month = jun,
  volume = {132},
  pages = {620--625},
  issn = {0004-0010},
  doi = {10.1001/archsurg.1997.01430300062013},
  abstract = {{$<$}h3{$>$}Objective:{$<$}/h3{$><$}p{$>$}To determine if blood transfusion is a consistent risk factor for postinjury multiple organ failure (MOF), independent of other shock indexes.{$<$}/p{$><$}h3{$>$}Design:{$<$}/h3{$><$}p{$>$}A 55-month inception cohort study ending on August 30, 1995. Data characterizing postinjury MOF were prospectively collected. Multiple logistic regression analysis was performed on 5 sets of data. Set 1 included admission data (age, sex, comorbidity, injury mechanism, Glasgow Coma Scale, Injury Severity Score, and systolic blood pressure determined in the emergency department) plus the amount of blood transfused within the first 12 hours. In the subsequent 4 data sets, other indexes of shock (early base deficit, early lactate level, late base deficit, and late lactate level) were sequentially added. Additionally, the same multiple logistic regression analyses were performed with early MOF and late MOF as the outcome variables.{$<$}/p{$><$}h3{$>$}Setting:{$<$}/h3{$><$}p{$>$}Denver General Hospital, Denver, Colo, is a regional level I trauma center.{$<$}/p{$><$}h3{$>$}Patients:{$<$}/h3{$><$}p{$>$}Five hundred thirteen consecutive trauma patients admitted to the trauma intensive care unit with an Injury Severity Score greater than 15 who were older than 16 years and who survived longer than 48 hours.{$<$}/p{$><$}h3{$>$}Interventions:{$<$}/h3{$><$}p{$>$}None.{$<$}/p{$><$}h3{$>$}Main Outcome Measures:{$<$}/h3{$><$}p{$>$}The relationship of blood transfusions and other shock indexes with the outcome variable, MOF.{$<$}/p{$><$}h3{$>$}Results:{$<$}/h3{$><$}p{$>$}A dose-response relationship between early blood transfusion and the later development of MOF was identified. Despite the inclusion of other indexes of shock, blood transfusion was identified as an independent risk factor in 13 of the 15 multiple logistic regression models tested; the odds ratios were high, especially in the early MOF models.{$<$}/p{$><$}h3{$>$}Conclusion:{$<$}/h3{$><$}p{$>$}Blood transfusion is an early consistent risk factor for postinjury MOF, independent of other indexes of shock.{$<$}/p{$><$}p{$>$}Arch Surg. 1997;132:620-625{$<$}/p{$>$}},
  file = {/home/architect/Zotero/storage/57HN7X87/596854.html},
  journal = {Arch Surg},
  language = {en},
  number = {6}
}
% == BibTeX quality report for mooreBloodTransfusionIndependent1997:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{ogdenDocumentThumbnailVisualizations1998,
  title = {Document Thumbnail Visualizations for Rapid Relevance Judgments: {{When}} Do They Pay Off?},
  shorttitle = {Document Thumbnail Visualizations for Rapid Relevance Judgments},
  booktitle = {The {{Seventh Text REtrieval Conference}} ({{TREC7}}). {{NIST}}},
  author = {Ogden, William and Davis, Mark and Rice, Sean},
  year = {1998},
  pages = {528--534},
  abstract = {this document retrieved?" Alternatively, the unique document viewer could have led to the superior performance. A fisheye view presents the area of current interest in normal scale but as distance from the interest area increases, information scale decreases. In Kaugar's document viewer, there could be multiple areas of interest, each defined by the presence of a search term in a sentence. These areas would be displayed in a normal font. Other sentences and paragraphs were shown in a smaller font. This allowed the user to very easily find and read relevant passages while ignoring intervening and mostly irrelevant text. Users could define new areas of interest in the document with a mouse click thereby returning those areas to a normal sized font. Because the documents were much smaller in size than the normal full sized scrolled view, much more of the relevant text fit within a single window on the computer screen perhaps making it easier to make relevance judgements.  Page 2},
  file = {/home/architect/Zotero/storage/QJWB5AK4/Ogden et al. - 1998 - Document thumbnail visualizations for rapid releva.pdf;/home/architect/Zotero/storage/Z55PSNG8/summary.html},
  keywords = {No DOI found}
}
% == BibTeX quality report for ogdenDocumentThumbnailVisualizations1998:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{ogdenImprovingCrosslanguageText2000,
  title = {Improving Cross-Language Text Retrieval with Human Interactions},
  booktitle = {Proceedings of the 33rd {{Annual Hawaii International Conference}} on {{System Sciences}}},
  author = {Ogden, W.C. and Davis, M.W.},
  year = {2000},
  month = jan,
  pages = {9 pp.-},
  issn = {null},
  doi = {10.1109/hicss.2000.926726},
  abstract = {Can we expect people to be able to get information from texts in languages they cannot read? We review two relevant lines of research bearing on this question and show how our results are being used in the design of a new Web interface for cross-language text retrieval. One line of research, "interactive IR", is concerned with the user interface issues for information retrieval systems such as how best to display the results of a text search. We review our current research, on "document thumbnail" visualizations, and discuss current Web conventions, practices and folklore. The other area of research, "Cross-Language Text Retrieval", is concerned with the design of automatic techniques, including Machine Translation, to retrieve texts in languages other than the language of the query. We review work we have done concerning query translation and multilingual text summarization. We then describe how these results are being applied and extended in the design a new demonstration interface, Keizai, an end-to-end, Web-based, cross-language text retrieval system. Beginning with an English query, the system will search Japanese and Korean Web data and display English summaries of the top ranking documents. A user should be able to accurately judge which foreign language documents are relevant to their information need and glean necessary information from the translation to schedule specific documents for human translation and subsequent analysis.},
  file = {/home/architect/Zotero/storage/PJ4II4VV/Ogden and Davis - 2000 - Improving cross-language text retrieval with human.pdf;/home/architect/Zotero/storage/7SVNAIJD/improving-crosslanguage-text-retrieval-with-human-interactions.html;/home/architect/Zotero/storage/W3T5KT6E/926726.html}
}
% == BibTeX quality report for ogdenImprovingCrosslanguageText2000:
% Missing required field 'publisher'

@inproceedings{olivaUnderstandingSensitivityFirst2020,
  title = {Understanding {{Sensitivity}}: A {{First Step Towards Automating Sensitivity Review}}},
  shorttitle = {Understanding {{Sensitivity}}},
  booktitle = {Archives, {{Access}} and {{AI}}: {{Working}} with {{Born}}-{{Digital}} and {{Digitised Archival Collections}}},
  author = {Oliva, Rebecca and Kim, Yunhyong},
  year = {2020},
  address = {{London, UK}},
  abstract = {Memory institutions face new challenges associated with curating data heritage in relation to ``technological requirements for specialist skills, hardware, and software to render digital objects'' (Harvey citation). In particular, it is generally accepted that manual approaches to processing digital records are becoming intractable, due to the volume of digital records stored by organisations (McDonald, Macdonald, and Ounis 2015).

Sensitivity review is one of the necessary processes by which archivists determine which records may be released to the public, redacted, or closed to the public. This paper will discuss the complex nature of sensitivity review, for example in relation to legal mandates, controversial subjects, and cultural differences, to present the many factors that influence archivists in the sensitivity review process. Keeping a record open when it contains sensitive information can mean that memory institutions are breaking the law (Sloyan 2016), but a risk-averse approach such as restricting access to records that have not yet been reviewed may result in reduced levels of service for users and obscure subtle cultural dynamics involved in sensitivity.

Lacking scalable approaches to tackle the complexity of sensitivity review and to mitigate such risks impedes archivists as they work to balance their responsibility to provide access to our data heritage with their duty to redact or close records that are sensitive. Some of these challenges could be addressed by incorporating automated or technology-assisted approaches.

This paper will propose a nuanced understanding of sensitivity within archives and demonstrate that sensitive data is characterised by its context as much as its content, opening up a discussion regarding the potential of machine learning, information retrieval and natural language processing techniques in developing scalable technology-assisted sensitivity review workflows.},
  file = {/home/architect/Zotero/storage/JZMXX44P/Oliva and Kim - 2020 - Understanding Sensitivity a First Step Towards Au.pdf;/home/architect/Zotero/storage/EG7SSRFP/193900.html},
  keywords = {No DOI found},
  language = {en-GB}
}
% == BibTeX quality report for olivaUnderstandingSensitivityFirst2020:
% Missing required field 'pages'
% Missing required field 'publisher'

@incollection{pasi_active_2018,
  title = {Active {{Learning Strategies}} for {{Technology Assisted Sensitivity Review}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {McDonald, Graham and Macdonald, Craig and Ounis, Iadh},
  editor = {Pasi, Gabriella and Piwowarski, Benjamin and Azzopardi, Leif and Hanbury, Allan},
  year = {2018},
  volume = {10772},
  pages = {439--453},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-76941-7_33},
  abstract = {Government documents must be reviewed to identify and protect any sensitive information, such as personal information, before the documents can be released to the public. However, in the era of digital government documents, such as e-mail, traditional sensitivity review procedures are no longer practical, for example due to the volume of documents to be reviewed. Therefore, there is a need for new technology assisted review protocols to integrate automatic sensitivity classification into the sensitivity review process. Moreover, to effectively assist sensitivity review, such assistive technologies must incorporate reviewer feedback to enable sensitivity classifiers to quickly learn and adapt to the sensitivities within a collection, when the types of sensitivity are not known a priori. In this work, we present a thorough evaluation of active learning strategies for sensitivity review. Moreover, we present an active learning strategy that integrates reviewer feedback, from sensitive text annotations, to identify features of sensitivity that enable us to learn an effective sensitivity classifier (0.7 Balanced Accuracy) using significantly less reviewer effort, according to the sign test (p {$<$} 0.01). Moreover, this approach results in a 51\% reduction in the number of documents required to be reviewed to achieve the same level of classification accuracy, compared to when the approach is deployed without annotation features.},
  file = {/home/architect/Zotero/storage/VI8KFINW/McDonald et al. - 2018 - Active Learning Strategies for Technology Assisted.pdf},
  isbn = {978-3-319-76940-0 978-3-319-76941-7},
  language = {en}
}
% == BibTeX quality report for pasi_active_2018:
% ? Title looks like it was stored in title-case in Zotero

@article{pedregosaScikitlearnMachineLearning2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  shorttitle = {Scikit-Learn},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  year = {2011},
  volume = {12},
  pages = {2825--2830},
  issn = {ISSN 1533-7928},
  file = {/home/architect/Zotero/storage/ZEHTDPA3/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf;/home/architect/Zotero/storage/R7MGB93H/pedregosa11a.html},
  journal = {J. Mach. Learn. Res.},
  keywords = {No DOI found},
  number = {Oct}
}
% == BibTeX quality report for pedregosaScikitlearnMachineLearning2011:
% ? Possibly abbreviated journal title J. Mach. Learn. Res.

@inproceedings{plattProbabilisticOutputsSupport1999,
  title = {Probabilistic {{Outputs}} for {{Support Vector Machines}} and {{Comparisons}} to {{Regularized Likelihood Methods}}},
  booktitle = {Advances in {{Large Margin Classifiers}}},
  author = {Platt, John C.},
  year = {1999},
  pages = {61--74},
  publisher = {{MIT Press}},
  abstract = {The output of a classifier should be a calibrated posterior probability to enable post-processing. Standard SVMs do not provide such probabilities. One method to create probabilities is to directly train a kernel classifier with a logit link function and a regularized maximum likelihood score. However, training with a maximum likelihood score will produce non-sparse kernel machines. Instead, we train an SVM, then train the parameters of an additional sigmoid function to map the SVM outputs into probabilities. This chapter compares classification error rate and likelihood scores for an SVM plus sigmoid versus a kernel method trained with a regularized likelihood error function. These methods are tested on three data-mining-style data sets. The SVM+sigmoid yields probabilities of comparable quality to the regularized maximum likelihood kernel method, while still retaining the sparseness of the SVM.},
  file = {/home/architect/Zotero/storage/JEWRW7Y5/Platt - 1999 - Probabilistic Outputs for Support Vector Machines .pdf;/home/architect/Zotero/storage/VNJ49SEL/summary.html},
  keywords = {No DOI found}
}
% == BibTeX quality report for plattProbabilisticOutputsSupport1999:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@misc{pstuffaCartesianFisheyeDistortion2019,
  title = {Cartesian {{Fisheye Distortion}}},
  author = {{pstuffa}},
  year = {2019},
  month = mar,
  abstract = {An Observable notebook by pstuffa.},
  file = {/home/architect/Zotero/storage/Z7RYRVKR/cartesian-fisheye-distortion.html},
  howpublished = {https://observablehq.com/@pstuffa/cartesian-fisheye-distortion},
  language = {en}
}
% == BibTeX quality report for pstuffaCartesianFisheyeDistortion2019:
% ? Title looks like it was stored in title-case in Zotero

@article{ribeiroWhyShouldTrust2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = feb,
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  archivePrefix = {arXiv},
  eprint = {1602.04938},
  eprinttype = {arxiv},
  file = {/home/architect/Zotero/storage/AZJK8PHN/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf},
  journal = {ArXiv160204938 Cs Stat},
  keywords = {â›” No DOI found,No DOI found},
  language = {en},
  primaryClass = {cs, stat}
}
% == BibTeX quality report for ribeiroWhyShouldTrust2016:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{ribeiroWhyShouldTrust2016a,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  pages = {1135--1144},
  publisher = {{Association for Computing Machinery}},
  address = {{San Francisco, California, USA}},
  doi = {10.1145/2939672.2939778},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  file = {/home/architect/Zotero/storage/J2EC9Z6B/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf;/home/architect/Zotero/storage/RGGJZU67/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf},
  isbn = {978-1-4503-4232-2},
  series = {{{KDD}} '16}
}
% == BibTeX quality report for ribeiroWhyShouldTrust2016a:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{robertsonDocumentLens1993,
  title = {The Document Lens},
  booktitle = {Proceedings of the 6th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology  - {{UIST}} '93},
  author = {Robertson, George G. and Mackinlay, Jock D.},
  year = {1993},
  pages = {101--108},
  publisher = {{ACM Press}},
  address = {{Atlanta, Georgia, United States}},
  doi = {10.1145/168642.168652},
  file = {/home/architect/Zotero/storage/7FYRIBAX/Robertson and Mackinlay - 1993 - The document lens.pdf;/home/architect/Zotero/storage/HM8XY4AM/robertson1993.html},
  isbn = {978-0-89791-628-8},
  language = {en}
}
% == BibTeX quality report for robertsonDocumentLens1993:
% ? Unsure about the formatting of the booktitle

@misc{romanRmnSklearndeltatfidf2019,
  title = {R-m-n/Sklearn-Deltatfidf},
  author = {Roman},
  year = {2019},
  month = dec,
  abstract = {DeltaTfidfVectorizer for scikit-learn. Contribute to r-m-n/sklearn-deltatfidf development by creating an account on GitHub.},
  copyright = {MIT}
}
% == BibTeX quality report for romanRmnSklearndeltatfidf2019:
% Missing required field 'howpublished'
% ? Title looks like it was stored in lower-case in Zotero

@incollection{sanchezDetectingSensitiveInformation2012,
  title = {Detecting {{Sensitive Information}} from {{Textual Documents}}: {{An Information}}-{{Theoretic Approach}}},
  shorttitle = {Detecting {{Sensitive Information}} from {{Textual Documents}}},
  booktitle = {Modeling {{Decisions}} for {{Artificial Intelligence}}},
  author = {S{\'a}nchez, David and Batet, Montserrat and Viejo, Alexandre},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Torra, Vicen{\c c} and Narukawa, Yasuo and L{\'o}pez, Beatriz and Villaret, Mateu},
  year = {2012},
  volume = {7647},
  pages = {173--184},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-34620-0_17},
  abstract = {Whenever a document containing sensitive information needs to be made public, privacy-preserving measures should be implemented. Document sanitization aims at detecting sensitive pieces of information in text, which are removed or hidden prior publication. Even though methods detecting sensitive structured information like e-mails, dates or social security numbers, or domain specific data like disease names have been developed, the sanitization of raw textual data has been scarcely addressed. In this paper, we present a general-purpose method to automatically detect sensitive information from textual documents in a domain-independent way. Relying on the Information Theory and a corpus as large as the Web, it assess the degree of sensitiveness of terms according to the amount of information they provide. Preliminary results show that our method significantly improves the detection recall in comparison with approaches based on trained classifiers.},
  file = {/home/architect/Zotero/storage/2R5WP6CJ/SÃ¡nchez et al. - 2012 - Detecting Sensitive Information from Textual Docum.pdf},
  isbn = {978-3-642-34619-4 978-3-642-34620-0},
  language = {en}
}
% == BibTeX quality report for sanchezDetectingSensitiveInformation2012:
% ? Title looks like it was stored in title-case in Zotero

@article{sanchezSensitiveDocumentRelease2017,
  title = {Toward Sensitive Document Release with Privacy Guarantees},
  author = {S{\'a}nchez, David and Batet, Montserrat},
  year = {2017},
  month = mar,
  volume = {59},
  pages = {23--34},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2016.12.013},
  abstract = {Privacy has become a serious concern for modern Information Societies. The sensitive nature of much of the data that are daily exchanged or released to untrusted parties requires that responsible organizations undertake appropriate privacy protection measures. Nowadays, much of these data are texts (e.g., emails, messages posted in social media, healthcare outcomes, etc.) that, because of their unstructured and semantic nature, constitute a challenge for automatic data protection methods. In fact, textual documents are usually protected manually, in a process known as document redaction or sanitization. To do so, human experts identify sensitive terms (i.e., terms that may reveal identities and/or confidential information) and protect them accordingly (e.g., via removal or, preferably, generalization). To relieve experts from this burdensome task, in a previous work we introduced the theoretical basis of C-sanitization, an inherently semantic privacy model that provides the basis to the development of automatic document redaction/sanitization algorithms and offers clear and a priori privacy guarantees on data protection; even though its potential benefits C-sanitization still presents some limitations when applied to practice (mainly regarding flexibility, efficiency and accuracy). In this paper, we propose a new more flexible model, named (C, g(C))-sanitization, which enables an intuitive configuration of the trade-off between the desired level of protection (i.e., controlled information disclosure) and the preservation of the utility of the protected data (i.e., amount of semantics to be preserved). Moreover, we also present a set of technical solutions and algorithms that provide an efficient and scalable implementation of the model and improve its practical accuracy, as we also illustrate through empirical experiments.},
  file = {/home/architect/Zotero/storage/GNXKAXW4/SÃ¡nchez and Batet - 2017 - Toward sensitive document release with privacy gua.pdf;/home/architect/Zotero/storage/IFXU3JXF/S0952197616302408.html},
  journal = {Engineering Applications of Artificial Intelligence},
  language = {en}
}
% == BibTeX quality report for sanchezSensitiveDocumentRelease2017:
% Missing required field 'number'

@article{schneiderToolsMethodsProcessing2017,
  title = {Tools and {{Methods}} for {{Processing}} and {{Visualizing Large Corpora}}},
  author = {Schneider, Gerold and {El-Assady}, Mennatallah and Lehmann, Hans Martin},
  year = {2017},
  abstract = {We present several approaches and methods which we develop or use to create workflows from data to evidence. They start with looking for specific items in large corpora, exploring overuse of particular items, and using off-the-shelf visualization such as GoogleViz. Second, we present the advanced visualization tools and pipelines which the Visualization Group at University of Konstanz is developing. After an overview, we apply statistical visualizations, Lexical Episode Plots and Interactive Hierarchical Modeling to the vast historical linguistics data offered by the Corpus of Historical American English (COHA), which ranges from 1800 to 2000. We investigate on the one hand the increase of noun compounds and visually illustrate correlations in the data over time. On the other hand we compute and visualize trends and topics in society from 1800 to 2000. We apply an incremental topic modeling algorithm to the extracted compound nouns to detect thematic changes throughout the investigated time period of 200 years. In this paper, we utilize various tailored analysis and visualization approaches to gain insight into the data from different perspectives.},
  file = {/home/architect/Zotero/storage/DD9IE4UD/45081.html},
  keywords = {No DOI found},
  language = {eng}
}
% == BibTeX quality report for schneiderToolsMethodsProcessing2017:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Title looks like it was stored in title-case in Zotero

@techreport{scottishgovernmentRedactingInformation2019,
  title = {Redacting {{Information}}},
  author = {{Scottish Government}},
  year = {2019},
  month = apr,
  file = {/home/architect/Zotero/storage/KS24FUXE/govscotdocument.pdf},
  number = {foi-19-00783}
}
% == BibTeX quality report for scottishgovernmentRedactingInformation2019:
% Missing required field 'institution'
% ? Title looks like it was stored in title-case in Zotero

@misc{ScottLundbergMicrosoft,
  title = {Scott {{Lundberg}}, {{Microsoft Research}} - {{Explainable Machine Learning}} with {{Shapley Values}} - \#{{H2OWorld}}},
  abstract = {This session was recorded in NYC on October 22nd, 2019.

Slides from the session can be viewed here: https://www.slideshare.net/secret/MBL...

Explainable Machine Learning with Shapley Values

Shapley values are popular approach for explaining predictions made by complex machine learning models. In this talk I will discuss what problems Shapley values solve, an intuitive presentation of what they mean, and examples of how they can be used through the `shap' python package.

Bio: I am a senior researcher at Microsoft Research. Before joining Microsoft, I did my Ph.D. studies at the Paul G. Allen School of Computer Science \&amp; Engineering of the University of Washington working with Su-In Lee. My work focuses on explainable artificial intelligence and its application to problems in medicine and healthcare. This has led to the development of broadly applicable methods and tools for interpreting complex machine learning models that are now used in banking, logistics, sports, manufacturing, cloud services, economics, and many other areas.},
  keywords = {\#nosource}
}
% == BibTeX quality report for ScottLundbergMicrosoft:
% Missing required field 'author'
% Missing required field 'howpublished'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@article{sloyanBorndigitalArchivesWellcome2016,
  title = {Born-Digital Archives at the {{Wellcome Library}}: Appraisal and Sensitivity Review of Two Hard Drives},
  shorttitle = {Born-Digital Archives at the {{Wellcome Library}}},
  author = {Sloyan, Victoria},
  year = {2016},
  month = jan,
  volume = {37},
  pages = {20--36},
  issn = {2325-7962},
  doi = {10.1080/23257962.2016.1144504},
  abstract = {Digital preservation has been an ongoing issue for the archival profession for many years, with research primarily being focused on long-term preservation and user access. Attention is now turning to the important middle stage: processing born-digital archives, which encompasses several key tasks such as appraisal, arrangement, description and sensitivity review. The Wellcome Library is developing scalable workflows for born-digital archival processing that deal effectively both with hybrid and purely born-digital archives. These workflows are being devised and tested using two hard drives deposited within the archives of two genomic researchers, Ian Dunham and Michael Ashburner. This paper examines two specific and interconnected stages of archival processing: appraisal and sensitivity review. It sets out the Wellcome Library's approach to appraisal using a combination of several appraisal methods, namely functional, technical and `bottom-up' appraisal. It also demonstrates how tools such as DROID can be used to streamline the process. The paper then goes on to explore the Wellcome Library's risk management-based approach to the sensitivity review of born-digital material, suggesting there is a viable balance to be struck between closing large record series as a precaution and sensitivity reviewing at a very granular level.},
  file = {/home/architect/Zotero/storage/VVDQXN36/Sloyan - 2016 - Born-digital archives at the Wellcome Library app.pdf;/home/architect/Zotero/storage/WPTRP4B9/23257962.2016.html},
  journal = {Arch. Rec.},
  number = {1}
}
% == BibTeX quality report for sloyanBorndigitalArchivesWellcome2016:
% ? Possibly abbreviated journal title Arch. Rec.

@article{stoffelDocumentThumbnailsVariable2012,
  title = {Document {{Thumbnails}} with {{Variable Text Scaling}}},
  author = {Stoffel, A. and Strobelt, H. and Deussen, O. and Keim, D. A.},
  year = {2012},
  volume = {31},
  pages = {1165--1173},
  issn = {1467-8659},
  doi = {10.1111/j.1467-8659.2012.03109.x},
  abstract = {Document reader applications usually offer an overview of the layout for each page as thumbnail views. Reading the text in these becomes impossible when the font size becomes very small. We improve the readability of these thumbnails using a distortion method, which retains a readable font size of interesting text while shrinking less interesting text further. In contrast to existing approaches, our method preserves the global layout of a page and is able to show context around important terms. We evaluate our technique and show application examples.},
  copyright = {\textcopyright{} 2012 The Author(s) Computer Graphics Forum \textcopyright{} 2012 The Eurographics Association and Blackwell Publishing Ltd.},
  file = {/home/architect/Zotero/storage/Y4HPTVC8/Stoffel et al. - 2012 - Document Thumbnails with Variable Text Scaling.pdf;/home/architect/Zotero/storage/EVCFILAM/stoffel2012.html;/home/architect/Zotero/storage/WCNMKX6X/j.1467-8659.2012.03109.html},
  journal = {Comput. Graph. Forum},
  language = {en},
  number = {3pt3}
}
% == BibTeX quality report for stoffelDocumentThumbnailsVariable2012:
% ? Possibly abbreviated journal title Comput. Graph. Forum
% ? Title looks like it was stored in title-case in Zotero

@article{strumbeljExplainingPredictionModels2014,
  title = {Explaining Prediction Models and Individual Predictions with Feature Contributions},
  author = {{\v S}trumbelj, Erik and Kononenko, Igor},
  year = {2014},
  month = dec,
  volume = {41},
  pages = {647--665},
  issn = {0219-3116},
  doi = {10.1007/s10115-013-0679-x},
  abstract = {We present a sensitivity analysis-based method for explaining prediction models that can be applied to any type of classification or regression model. Its advantage over existing general methods is that all subsets of input features are perturbed, so interactions and redundancies between features are taken into account. Furthermore, when explaining an additive model, the method is equivalent to commonly used additive model-specific methods. We illustrate the method's usefulness with examples from artificial and real-world data sets and an empirical analysis of running times. Results from a controlled experiment with 122 participants suggest that the method's explanations improved the participants' understanding of the model.},
  file = {/home/architect/Zotero/storage/A2XETQQ8/Å trumbelj and Kononenko - 2014 - Explaining prediction models and individual predic.pdf;/home/architect/Zotero/storage/AULXGLRT/10.1007@s10115-013-0679-x.html},
  journal = {Knowl Inf Syst},
  language = {en},
  number = {3}
}

@book{SYSTEMSSOFTWARESERVICES2019,
  title = {{{SYSTEMS}}, {{SOFTWARE AND SERVICES PROCESS IMPROVEMENT 26TH EUROPEAN}}.},
  year = {2019},
  publisher = {{SPRINGER NATURE}},
  address = {{S.l.}},
  file = {/home/architect/Zotero/storage/D6DVS92B/2019 - SYSTEMS, SOFTWARE AND SERVICES PROCESS IMPROVEMENT.pdf},
  isbn = {978-3-030-28004-8},
  language = {en},
  note = {OCLC: 1107563133}
}
% == BibTeX quality report for SYSTEMSSOFTWARESERVICES2019:
% Missing required field 'author'
% ? Title looks like it was stored in title-case in Zotero

@techreport{thenationalarchivesRedactionToolkitPaper2016,
  title = {Redaction Toolkit for Paper and Electronic Documents: {{Editing}} Exempt Information from Paper and Electronicdocuments Prior to Release},
  shorttitle = {Redaction Toolkit for Paper and Electronic Documents},
  author = {{The National Archives}},
  year = {2016},
  month = apr,
  pages = {23},
  institution = {{The National Archives}},
  file = {/home/architect/Zotero/storage/XX6GUJPT/2016 - Redaction toolkit.pdf},
  language = {en}
}

@incollection{toughScopeAppetiteTechnologyassisted2018,
  title = {The Scope and Appetite for Technology-Assisted Sensitivity Reviewing of Born-Digital Records in a Resource Poor Environment: A Case Study from {{Malawi}}},
  shorttitle = {The Scope and Appetite for Technology-Assisted Sensitivity Reviewing of Born-Digital Records in a Resource Poor Environment},
  booktitle = {Handbook of {{Research}} on {{Heritage Management}} and {{Preservation}}},
  author = {Tough, Alistair},
  editor = {Ngulube, Patrick},
  year = {2018},
  month = feb,
  pages = {175--182},
  publisher = {{IGI Global}},
  abstract = {Concerns about sensitive content in born-digital records seem to be a major factor in inhibiting the deposit
of public records in dedicated digital repositories in Western countries. These concerns are much
exacerbated by the changed nature of the process of reviewing records. The University of Glasgow,
working in collaboration with the Foreign and Commonwealth Office, received funding to investigate the
technology-assisted sensitivity reviewing of born-digital records. As part of this research, some preliminary
research in a commonwealth country in Sub-Saharan Africa was carried out. The research, reported in
this chapter, was carried out in Malawi by the late Dr. Mathews J. Phiri. He found that already there
is a real, albeit limited, demand for technology-assisted sensitivity reviewing of born-digital records in
Malawi. The available evidence suggests that within the next decade there is likely to be an increase in
the need for effective means of assessing sensitivity in born-digital records.},
  file = {/home/architect/Zotero/storage/9KSY9IWE/150844.html},
  isbn = {978-1-5225-3137-1},
  language = {en}
}

@misc{UnifiedApproachInterpreting,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}} - {{NIPS}} 2017},
  abstract = {NIPS 2017 Short},
  keywords = {\#nosource}
}
% == BibTeX quality report for UnifiedApproachInterpreting:
% Missing required field 'author'
% Missing required field 'howpublished'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@book{walkerSystemsSoftwareServices2019,
  title = {Systems, {{Software}} and {{Services Process Improvement}}: 26th {{European Conference}}, {{EuroSPI}} 2019, {{Edinburgh}}, {{UK}}, {{September}} 18\textendash{}20, 2019, {{Proceedings}}},
  shorttitle = {Systems, {{Software}} and {{Services Process Improvement}}},
  author = {Walker, Alastair and O'Connor, Rory V. and Messnarz, Richard},
  year = {2019},
  month = oct,
  publisher = {{Springer Nature}},
  abstract = {This volume constitutes the refereed proceedings of the 26th European Conference on Systems, Software and Services Process Improvement, EuroSPI conference, held in Edinburgh, Scotland, in September 2019. The 18 revised full papers presented were carefully reviewed and selected from 28 submissions. They are organized in topical sections: Visionary Papers, SPI and Safety and Security, SPI and Assessments, SPI and Future Qualification \& Team Performance, and SPI Manifesto and Culture. The selected workshop papers are also presented and organized in following topical sections: GamifySPI, Digitalisation of Industry, Infrastructure and E-Mobility. -Best Practices in Implementing Traceability. -Good and Bad Practices in Improvement. -Functional Safety and Cybersecurity. -Experiences with Agile and Lean. -Standards and Assessment Models. -Team Skills and Diversity Strategies. -Recent Innovations.},
  file = {/home/architect/Zotero/storage/5CWHE57C/Walker et al. - 2019 - Systems, Software and Services Process Improvement.pdf},
  googlebooks = {v0yuDwAAQBAJ},
  isbn = {978-3-030-28005-5},
  language = {en}
}
% == BibTeX quality report for walkerSystemsSoftwareServices2019:
% ? Title looks like it was stored in title-case in Zotero

@article{wenThunderSVMFastSVM2018,
  title = {{{ThunderSVM}}: A Fast {{SVM}} Library on {{GPUs}} and {{CPUs}}},
  shorttitle = {{{ThunderSVM}}},
  author = {Wen, Zeyi and Shi, Jiashuai and Li, Qinbin and He, Bingsheng and Chen, Jian},
  year = {2018},
  month = jan,
  volume = {19},
  pages = {797--801},
  issn = {1532-4435},
  abstract = {Support Vector Machines (SVMs) are classic supervised learning models for classification, regression and distribution estimation. A survey conducted by Kaggle in 2017 shows that 26\% of the data mining and machine learning practitioners are users of SVMs. However, SVM training and prediction are very expensive computationally for large and complex problems. This paper presents an efficient and open source SVM software toolkit called ThunderSVM which exploits the high-performance of Graphics Processing Units (GPUs) and multi-core CPUs. ThunderSVM supports all the functionalities--including classification (SVC), regression (SVR) and one-class SVMs--of LibSVM and uses identical command line options, such that existing LibSVM users can easily apply our toolkit. ThunderSVM can be used through multiple language interfaces including C/C++, Python, R and MATLAB. Our experimental results show that ThunderSVM is generally an order of magnitude faster than LibSVM while producing identical SVMs. In addition to the high efficiency, we design our convex optimization solver in a general way such that SVC, SVR, and one-class SVMs share the same solver for the ease of maintenance. Documentation, examples, and more about ThunderSVM are available at https://github.com/zeyiwen/thundersvm.},
  file = {/home/architect/Zotero/storage/RJTUIE2U/Wen et al. - 2018 - ThunderSVM a fast SVM library on GPUs and CPUs.pdf},
  journal = {J. Mach. Learn. Res.},
  keywords = {No DOI found},
  number = {1}
}
% == BibTeX quality report for wenThunderSVMFastSVM2018:
% ? Possibly abbreviated journal title J. Mach. Learn. Res.

@article{wenThunderSVMFastSVM2018a,
  title = {{{ThunderSVM}}: A Fast {{SVM}} Library on {{GPUs}} and {{CPUs}}},
  shorttitle = {{{ThunderSVM}}},
  author = {Wen, Zeyi and Shi, Jiashuai and Li, Qinbin and He, Bingsheng and Chen, Jian},
  year = {2018},
  month = jan,
  volume = {19},
  pages = {797--801},
  issn = {1532-4435},
  abstract = {Support Vector Machines (SVMs) are classic supervised learning models for classification, regression and distribution estimation. A survey conducted by Kaggle in 2017 shows that 26\% of the data mining and machine learning practitioners are users of SVMs. However, SVM training and prediction are very expensive computationally for large and complex problems. This paper presents an efficient and open source SVM software toolkit called ThunderSVM which exploits the high-performance of Graphics Processing Units (GPUs) and multi-core CPUs. ThunderSVM supports all the functionalities--including classification (SVC), regression (SVR) and one-class SVMs--of LibSVM and uses identical command line options, such that existing LibSVM users can easily apply our toolkit. ThunderSVM can be used through multiple language interfaces including C/C++, Python, R and MATLAB. Our experimental results show that ThunderSVM is generally an order of magnitude faster than LibSVM while producing identical SVMs. In addition to the high efficiency, we design our convex optimization solver in a general way such that SVC, SVR, and one-class SVMs share the same solver for the ease of maintenance. Documentation, examples, and more about ThunderSVM are available at https://github.com/zeyiwen/thundersvm.},
  file = {/home/architect/Zotero/storage/2S8C7WPQ/Wen et al. - 2018 - ThunderSVM a fast SVM library on GPUs and CPUs.pdf},
  journal = {J. Mach. Learn. Res.},
  keywords = {No DOI found},
  number = {1}
}
% == BibTeX quality report for wenThunderSVMFastSVM2018a:
% ? Possibly abbreviated journal title J. Mach. Learn. Res.

@misc{zhongVincentdchanReactfisheye2019,
  title = {Vincentdchan/React-Fisheye},
  author = {Zhong},
  year = {2019},
  month = nov,
  abstract = {React-fisheye is a react component implements fisheye effect.},
  keywords = {\#nosource}
}
% == BibTeX quality report for zhongVincentdchanReactfisheye2019:
% Missing required field 'howpublished'
% ? Title looks like it was stored in lower-case in Zotero


