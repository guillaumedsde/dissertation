% !TeX root = ./dissertation.tex
% !TeX spellcheck = en-GB
% REMEMBER: You must not plagiarise anything in your report. Be extremely careful.

% load draft.tex if it exists
\InputIfFileExists{draft.tex}{}{}

% fallback to "final" version by default
\ifdefined\version
\else
\def\version{final}
\fi

\documentclass[\version]{l4proj}
    
%
% put any additional packages here
%

% Disable compression if in draft mode
\usepackage{ifdraft}
\ifdraft{
    \special{dvipdfmx:config z 0}
}{
    \special{dvipdfmx:config z 9}
}

\usepackage{float}

\usepackage{wrapfig}

\usepackage[justification=centering]{caption}
% \captionsetup{belowskip=1pt,aboveskip=4pt}

\usepackage{adjustbox}

% Referencing 
\setmainlanguage{english}
\usepackage[backend=biber,date=year,alldates=year,urldate=short,bibstyle=apa,maxcitenames=2,uniquelist=false,citestyle=authoryear,block=ragged]{biblatex}
\DeclareLanguageMapping{english}{english-apa}
\AtEveryCite{%
  \clearfield{month}%
  \clearfield{day}%
  \clearfield{labelmonth}%
  \clearfield{labelday}%
  \clearfield{labelendmonth}%
  \clearfield{labelendday}%
}

\bibliography{l4proj.bib}

% break bilbatex reference URL on number
\setcounter{biburlnumpenalty}{9000}

\usepackage{enumitem}

\usepackage{tocvsec2}

\usepackage[outputdir=build,chapter=true]{minted}

\setminted{breaklines=true}
\setminted{linenos=true}
\setminted{tabsize=2}
\setminted{fontsize=\footnotesize}


\usepackage{pdfpages}

\usepackage{epigraph}

% length of epigraph
\setlength\epigraphwidth{12cm}
% remove epigraph bar
\setlength\epigraphrule{0pt}

% \renewcommand{\epigraphsize}{\Huge}

\usepackage{lettrine}

% license
\usepackage[
    type={CC}, 
    modifier={by-nc-sa},
    version={4.0},
]{doclicense}

\usepackage{csquotes}
 
\usepackage{multicol}

\newenvironment{longlisting}{\captionsetup{type=listing}}{}


\begin{document}

%==============================================================================
%% METADATA
\title{Why is This Sensitive? \\ Visualising Important Sensitivity Classification Features}
\author{Guillaume de Susanne d'Epinay}
\date{\today}
\def\email{guillaume |at| desusanne |dot| com}

\maketitle

%==============================================================================
%% ABSTRACT
\begin{abstract}
    Every abstract follows a similar pattern. Motivate; set aims; describe work; explain results.
    \vskip 0.5em
    ``XYZ is bad. This project investigated ABC to determine if it was better.
    ABC used XXX and YYY to implement ZZZ. This is particularly interesting as XXX and YYY have
    never been used together. It was found that
    ABC was 20\% better than XYZ, though it caused rabies in half of subjects.''
\end{abstract}

%==============================================================================

% EDUCATION REUSE CONSENT FORM
% If you consent to your project being shown to future students for educational purposes
% then insert your name and the date below to  sign the education use form that appears in the front of the document. 
% You must explicitly give consent if you wish to do so.
% If you sign, your project may be included in the Hall of Fame if it scores particularly highly.
%
% Please note that you are under no obligation to sign 
% this declaration, but doing so would help future students.
%
%\def\consentname {My Name} % your full name
%\def\consentdate {20 March 2018} % the date you agree
%

\def\consentname{Guillaume de Susanne d'Epinay}
\def\consentdate{\today}

\educationalconsent

\renewcommand{\abstractname}{Acknowledgements}
\begin{abstract}
    I wish to express my gratitude to both my supervisors, Craig Macdonald and Graham McDonald for their support, guidance and feedback throughout the year.

    I would also like to thank the expert panel for their invaluable feedback on this project.
\end{abstract}

\newpage

\vspace*{\fill}

\epigraph{\Large{``Transparency is for those who carry out public duties and exercise public power. Privacy is for everyone else''}}{\fullcite[178]{greenwaldNoPlaceHide2014}}

\vspace*{\fill}

%==============================================================================
\tableofcontents

%==============================================================================
%% Notes on formatting
%==============================================================================
% The first page, abstract and table of contents are numbered using Roman numerals and are not
% included in the page count. 
%
% From now on pages are numbered
% using Arabic numerals. Therefore, immediately after the first call to \chapter we need the call
% \pagenumbering{arabic} and this should be called once only in the document. 
%
% Do not alter the bibliography style.
%
% The first Chapter should then be on page 1. You are allowed 40 pages for a 40 credit project and 30 pages for a 
% 20 credit report. This includes everything numbered in Arabic numerals (excluding front matter) up
% to but excluding the appendices and bibliography.
%
% You must not alter text size (it is currently 10pt) or alter margins or spacing.
%
%
%==================================================================================================================================
%
% IMPORTANT
% The chapter headings here are **suggestions**. You don't have to follow this model if
% it doesn't fit your project. Every project should have an introduction and conclusion,
% however. 
%
%==================================================================================================================================



\chapter{Introduction}

% reset page numbering. Don't remove this!
\pagenumbering{arabic}

\section{The need for Technology Assisted Sensitivity Review}

\lettrine[lines=3,nindent=0em]{O}{n} June 2nd 2013, in a busy Hongkongese commercial district, tens of thousands of neatly organized documents traded hands from a soon to be ex-NSA Employee to two American journalists refuged in one of the 492 rooms of the Mira Hotel.
The Snowden archive handed over that day, ``stunning both in size and scope'' ``had been produced by virtually every unit and subdivision within the sprawling agency, and [\ldots] closely aligned foreign intelligence agencies'' \autocite[77]{greenwaldNoPlaceHide2014}.
Months later, even after developing indexing and search tools, Greenwald and his team were still pouring over the mountain of documents that had been disclosed in order to redact information that could cause harm to privacy or national interest \autocite{greenwaldFactsHowNSA2014}.
Consequently, the review process was not without flaws with misredactions actually revealing such information \autocite{oliverGovernmentSurveillanceLast2015}.
Thus, the many hands that worked on the Snowden archive were confronted with a task only a fraction the scale of the work of today's sensitivity reviewers.

In the United Kingdom (UK), the \textcite{PublicRecordsAct1958} created what is today known as The National Archives (TNA), a public institution keeper of governmental records that the act stipulates, should be released within thirty years of their creation.
The \textcite{FreedomInformationAct2000} (FOIA) later introduced a public ``right of access'' through requests to information held by public authorities.
Most recently, the \textcite{ConstitutionalReformGovernance2010} reduced the time to a record's publication from 30 years to 20 years after their creation.

These documents have to be sensitivity reviewed in order to redact information exempted from disclosure by the FOIA.
This includes, amongst other exceptions, personal information (Section 40 ``S40'') and information deemed harmful to international relations (Section 27 ``S27''). %, confidential information (Section 41 ``S41'') and commercial interests (Section 43 ``S43'').
% Figure \ref{fig:fracking_report} illustrates and example of such redacted exemptions in \textcite{StateUKShale2016}.
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.6\linewidth]{figures/fracking_report_22.pdf}
%     \caption{Heavily redacted excerpt from \textcite[22]{StateUKShale2016}, a report containing S41 and S43 FOIA exemptions but released in redacted form by order of \textcite{judgeshanksCabinetOfficeICO2018} following a Greenpeace FOIA request}\label{fig:fracking_report}
% \end{figure}
This sensitivity review process is particularly time consuming because it has to be thorough: everything that is released has to be reviewed.
The scale of this task cannot be understated: in 2014 the total number of documents pending release stood at a staggering 817,615 records across all British Government departments \autocite{allanRecordsReview2014,thenationalarchivesRecordTransferReport2014}.
Moreover, this number only represents the records that have been accounted for, only within departments that keep track of these figures and must legally transfer their record to TNA.
It is safe to say that Greenwald's team's work on the Snowden archive is dwarfed in scale by the British government's Herculean backlog of records, challenging the established principle of thorough document review.

In order to preserve this paradigm, such magnitude of records calls for digital tools to accelerate the sensitivity review process.
Otherwise, the risk is for Public Institutions to become overwhelmed by their review backlog forcing them to resort to blanket closure of records for extended periods of time (up to a century), delegating the handling of sensitive information to the erosion of time and preventing rightful public access.

\section{Objective}

Our aim is to contribute to the research into these Technology Assisted Review (TAR) tools to assist sensitivity reviewers.
Notably, we seek to expand upon the work in this emerging field which has produced Machine Learning classifiers focused on identifying sensitive documents to prioritize and better target reviewing resources thus processing documents faster.
A lot of effort has been dedicated to these Machine Learning algorithms: some work has focused on evaluating their impact at reducing review time, but there is not enough research on visualizing these sensitivities to display them in an understandable user interface (UI).

We want to introduce an approach to displaying sensitive information within documents based on these Machine Learning classifiers.
Our objective is to design, build and evaluate a proof of concept fullstack application for classifying, visualizing and redacting potentially sensitive documents.
Ideally, we aim to decrease the time reviewers spend on documents and allow them to more accurately identify sensitive records.

\section{Outline}

To proceed, we start with a review of the literature on the application of TAR to sensitivity review.
Then we explore some of the works on the use of TAR in the legal profession, notably in legal discovery.
We also assess some of the recent implementations of explanations for Machine Learning Classifiers' predictions.
Finally we survey some of the research into visualization of dense information spaces.
% TODO dense information spaces might a little bullshitty?
We then evaluate similar or otherwise related products, picking out key interesting features as well as antipatterns we wish to avoid.

In order to build an application, we delineate a scope, clarifying our end product, we also establish a set of requirements to prioritize features to implement.
We elaborate on the testing methodology for a user study and lastly we draw up wireframe diagrams to visualize a draft layout of components in the application a well as to get a better sense of feature priorities.

We then discuss our application design process and summarize our technology stack. We elaborate on our choice of libraries, the reasons for our final picks as well alternative options and why we did not elect them.

Afterwards, we detail some of the important aspects of the software, highlighting key sections of the code as well as our Repository and Continuous Integration setup.
We explain the issues we ran into, from data storage concerns to frontend implementation struggles as well as Machine Learning classifier optimization and debugging.

Lastly before concluding, we detail the findings from an evaluation by an expert panel, as well as the user evaluation we would have conducted had the COVID-19 pandemic not rendered it infeasible.


%==================================================================================================================================
\chapter{Background}

\section{Literature Review}\label{section:lit_review}

\subsection{Automated classification of sensitive enterprise documents}

Early works on the application of Text Analytics for identifying sensitive information focused on personal and client information for use in enterprise.
\textcite{cumbyMachineLearningBased2011} for instance developed a addon for identifying this information in Word documents.
Given a set of documents associated with a sensitivity category, they trained a Machine Learning classifier for sensitive documents.
They then devised a framework for removing names, social security numbers or a client identity (company name) from these documents while preserving their utility.

That same year \textcite{hartTextClassificationData2011} also published their work on applying Machine Learning classification for identifying sensitive corporate documents.
They create a corpus from publicly available corporate documents and determine that a linear Support Vector Classifier (SVC) along with further processing yielded the best results for automated binary classification of documents as ``secret'' or not.

\textcite{sanchezDetectingSensitiveInformation2012} followed up on this work by creating a semi-supervised system for identifying private information within celebrities' personal Wikipedia pages.
They compare using an Information Content measure (specific terms contain ``more information'' than generic ones) with named entity recognition on textual tokens with Part of Speech tagging and syntactic parsing to identify sensitive information.

\subsection{TAR of sensitive governmental records}

\textcite{mcdonaldClassifierDigitalSensitivity2014} was the first application of TAR for identifying sensitivities in governmental records for which they established an initial Machine Learning sensitive document classifier.
They combined the textual content of these documents with sentiment analysis, a custom country risk score, entity and name recognition in order to achieve a balanced accuracy (BAC) of 0.63 for FOIA S27 and 0.74 for S40.

As a follow up, \textcite{berardiSemiAutomatedTextClassification2015} established that BAC and the \(F_{2}\) measure are best suited when evaluating sensitivity classifiers due to the predominance of recall over precision: a released sensitive document is potentially more problematic than an excessive closure of information.
As such, rather than an automated \textit{replacement}, they frame the solution as an \textit{aid} to sensitivity reviewers.
They demonstrate the advantage of a utility-theoric approach to maximizing reviewer effectiveness.
Specifically they focus on the human reviewer validation of the automated classifier's labelling: prioritizing the review of document with a high probability of an ``automated misclassification''.

\textcite{mcdonaldUsingPartofSpeechNgrams2015} expanded on the idea and improved classification with Part-of-Speech tagging of large N-grams filtering out those which are not characteristic sensitivity by calculating n-grams' ``sensitivity load'' \autocite[2]{mcdonaldUsingPartofSpeechNgrams2015}.
This results in 0.73 BAC and 0.51 \(F_{2}\) when identifying a subset of S27 FOIA exemptions: ``information supplied in confidence''.

This work was furthered with an in depth study of SVM kernel functions' performance for POS sequence classification \autocite{mcdonaldStudySVMKernel2017}.
The paper also combined the POS sequence classification with full text classification to obtain a maximum \(F_{2}\) measure of 0.46 and a maximum BAC of 0.64 for S27 and S40 sensitive documents combined.
\textcite{mcdonaldEnhancingSensitivityClassification2017} further expanded this model on the same dataset, obtaining a BAC of 0.71 and \(F_{2}\) of 0.54 using Word embeddings and semantic features.

\textcite{mcdonaldActiveLearningStrategies2018} implemented ``Active Learning'' of reviewer annotations for dynamically prioritizing documents in a collection in order to maximize the automated classifier's effectiveness minimal manual review of documents.
They achieved a BAC of 0.7 on the same collection as \textcite{mcdonaldStudySVMKernel2017,mcdonaldEnhancingSensitivityClassification2017} with a 51\% reduction in the number of manually reviewed documents.

\textcite{mcdonaldHowSensitivityClassification2019} conducted a user study to evaluate the impact of classification effectiveness on reviewers' performance. With the aid of a simple interface, they asked reviewers to identify S27 and S40 FOIA sensitivities with and without sensitivity classifier predictions.
They concluded that, while participants' identified sensitivities were randomly correct, participants aided with medium and perfect classifier prediction performance respectively obtained a BAC of 0.69 and 0.8.

\subsection{``Predictive Coding'' or \textit{TAR} for legal electronic discovery}

Ongoing research in the area of Judicial practice has also applied the concept of TAR to the process of legal discovery.
This research has coined the term ``Predictive Coding'' \autocite{carrollGrossmancormackGlossaryTechnologyassisted2013} as an industry specific use of TAR in the legal practice, particularly for ``legal discovery'', the pre-trial exchange of evidence by both parties.
Parallels can be drawn with the task of sensitivity review, specifically with the process of identifying privileged information in a legal discovery corpus.

\textcite{grossmanTechnologyAssistedReviewEDiscovery2010} compared the performances of a manual and TAR aided eDiscovery process with data from the various teams of the TREC 2009 Legal Track \autocite{hedinOverviewTREC2009}.
They conclude that TAR methods lead to more accurate eDiscovery relevance estimations with significantly lower effort than with an exhaustive manual discovery process.

One of these teams, \textcite{cormackMachineLearningInformation2009}, constructed various training sets for a given list of discovery topics from a corpus of eDiscovery documents using IR techniques (a search engine).
They then fitted logistic regression classifiers on these training sets to classify the rest of the collection as relevant or not for each topic.
Ultimately, this team obtained the best F1 measure averaged across all topics \autocite{hedinOverviewTREC2009}.
Moreover, ``by all measures, the average efficiency and effectiveness of the five technology-assisted reviews surpasses that of the five manual reviews'' \autocite[p.~43]{grossmanTechnologyAssistedReviewEDiscovery2010}.

The task at hand is different from the review of sensitive documents.
Firstly, the implications of missing a relevant document in sensitivity review can be far greater than in legal discovery.
The works we have cited above only manually reviewed fractions of their collection, accepting a level of ``misclassification'' that might not be tolerable in sensitivity review.
Thus, the task of identifying ``privileged'' information in legal discovery documents is the one that comes closest to that of sensitivity review \autocite{berardiSemiAutomatedTextClassification2015} because of the skewed importance towards recall rather than precision.
In fact, research in Technology Assisted Sensitivity Review (TASR) has determined that ``all government documents will continue to be manually sensitivity reviewed for the foreseeable future'' \autocite[1]{mcdonaldHowSensitivityClassification2019}.
Early on in TSAR research, the emphasis has been on designing systems that support reviewers rather than ones that replace them \autocite{mcdonaldClassifierDigitalSensitivity2014}.

\section{Explanations in Machine Learning}

This speaks to the level of trust we confer to these systems, while they might yield impressive results, we are not willing to delegate tasks we deem crucial to them entirely.

This lack of trust is attributed to our lack of complete understanding of the decision process of such automated systems which are often referred to as ``black boxes'' \autocite{ribeiroWhyShouldTrust2016}.
Recent research into Explanation techniques for Machine Learning models have noted this need for interpretability.

\textcite{ribeiroWhyShouldTrust2016} devised a method called ``Local Interpretable Model-agnostic Explanations'' (LIME).
LIME provides explanations for individual predictions from any model.
It does so by sampling $k$ features from a classifier's input and evaluating the behaviour of a linear model trained with the $k$ sampled features in the vicinity of the actual prediction.
This allows them to calculate feature weights denoting ``how much'' each contributed to the actual model's prediction by randomly sampling them from the original model.
Thus, they generated explanations for predictions that are simpler than the input data (e.g. words instead of documents).
This makes it easier to understand the decision of a model without understanding the vectorial operations of classifier algorithms .
As far as the user knows, the model can stay a ``black box'' because he now sees a simplified feature space with numeric values for each feature allowing him to better understand the model's decision.

\textcite{lundbergUnifiedApproachInterpreting2017} expanded on this work with a technique called ``SHapley Additive exPlanations'' (SHAP) which calculates predictions for all possible combinations of features.
They used Shapley values \autocite{shapleyNotesNPersonGame1951}, a Game Theory concept for calculating an individual's contribution to a group, in order to compute a score for each feature's contribution to the classification.
\textcite{lundbergUnifiedApproachInterpreting2017} formally demonstrated that SHAP values are the \textit{only} possible solution for an explanation model to be locally accurate, consistent and also respect \textit{missingness} (no attribution of score to absent features).
Therefore they also showed that other explanation techniques like LIME or DeepLIFT must lack one of the above.
While some approximation techniques and optimizations on certain ML models mitigate this, its main drawback is that it is significantly more complex to compute since it explores all possible input feature combinations.

The main repositories for both these implementations, provide illustrated examples of these explanations \autocite{lundbergSlundbergShap2020,ribeiroMarcotcrLime2020}.
LIME's approach to textual feature is with coloured highlighting and varying opacity depending on the feature's contribution.
The SHAP library takes a different approach with a ``force plot'' of feature weights which summarizes the most important feature contributions in the document.
These depictions raise the core issue of this project: visualizing document sensitivities.

\section{Approaches to document visualization}

To address this, we review some of the literature on document visualization, noting our key inspirations and takeaways.
Our main focus is on summarizing the dense information spaces into a visualization in order to provide an overview of how a document might be sensitive as well as where it is sensitive.

\pagebreak

\subsection{Lexical Episode Plots}
%
\begin{wrapfigure}{r}{0.45\textwidth}
    \centering
    \begin{subfigure}[c]{0.23\textwidth}
        \centering
        \small
        \includegraphics[width=\linewidth]{images/document_visualization/topic-overview.png}
        \caption{Lexical Episode Plots}\label{fig:lexical_plot}
    \end{subfigure}
    \hspace{0.01\textwidth}
    \begin{subfigure}[c]{0.19\textwidth}
        \centering
        \small
        \includegraphics[width=\linewidth]{images/related_products/vscode_minimap.png}
        \caption{VSCode Minimap}\label{fig:vscode_minimap}
    \end{subfigure}
    \caption{Document Overviews}\label{fig:test-modes}
    \vspace{-40pt}
\end{wrapfigure}

We start with a document overview displaying highlights beside the document outline also showing the ``topic'' of the highlight (Figure~\ref{fig:lexical_plot}), it is called ``Lexical Episode Plots'' \autocite{el-assadyVisArgueVisualText2016,goldExploratoryTextAnalysis2015}.

This is reminiscent of code overviews in text editors, especially those aimed at editing code, which provide an overview of the ``shape'' of the text with syntax highlighted by colour to allow fast navigation between sections of source code like what \textcite{MicrosoftVscode2020} implements (Figure~\ref{fig:vscode_minimap}).

An old Information Retrieval system, J24 \autocite[7]{ogdenDocumentThumbnailVisualizations1998}, also implements a similar feature by highlighting query terms in a document outline.
This shows that perhaps, these sort of overviews could also be used for previewing a document, for example as a popup when a document is hovered on within a collection.

\subsection{Varying font size}

\begin{wrapfigure}{l}{0.4\textwidth}
    \includegraphics[width=\linewidth]{images/document_visualization/font-size.png}
    \caption{Adjusting font size}\label{fig:font-size}
    \vspace{-5pt}
\end{wrapfigure}

\textcite{stoffelDocumentThumbnailsVariable2012} implement a combination of text highlighting along with font size variations for creating distorted thumbnails for previewing important document features (Figure~\ref{fig:font-size}).
It is not certain this would be a good thumbnail, the highlighted portions definitely show where key features are in the document, but a lot of the document structure is lost.
However varying font size within a \textit{reasonable range} would be an interesting visualization for feature importance in a prediction, similarly to what the aforementioned LIME library displays.

\subsection{Distortion}

\begin{wrapfigure}{r}{0.4\textwidth}
    \includegraphics[width=\linewidth]{images/document_visualization/different-fisheyes.png}
    \caption{Fisheye views \autocite{baudischFishnetFisheyeWeb2004}
    }\label{fig:fisheyes}
    \vspace{-10pt}
\end{wrapfigure}

Text distortion techniques have many names and variants ``Document Lens'' \autocite{robertsonDocumentLens1993}, ``Fisheye'' view \autocite{greenbergFisheyeTextEditor1996} or ``hybrid continuous zoom'' \autocite{bartramContinuousZoomConstrained1995} with different implementations (see Figure~\ref{fig:fisheyes}) they are part of a concept called ``Bifocal Display'' \autocite{apperleyBifocalDisplay}.

For example, \textit{Circular Fisheye Distortion} is a ``magnifying glass'' effect visible in \textcite{bostockFisheyeGrid2019}. \textit{Cartesian Distortion} ``magnifies continuously so as to avoid local minification'' \autocite{bostockFisheyeDistortion2012}. The effect can also be limited to only one axis (vertical or horizontal) as seen in \textcite{pstuffaCartesianFisheyeDistortion2019}. In our case, Vertical Cartesian Distortion on a per line basis seems like an interesting visualization for document overview with emphasis on hovered lines. There is a Javascript implementation of this effect \autocite{pstuffaCartesianFisheyeDistortion2019}, it also seems like there is a ReactJS specific implementation of the text fisheye effect \autocite{zhongVincentdchanReactfisheye2019}.

\section{Related products}

\subsection{PDF editors}

\begin{wrapfigure}{r}{0.45\textwidth}
    \includegraphics[width=\linewidth]{images/related_products/adobe_redaction.png}
    \caption{Acrobat redaction workflow}\label{fig:adobe-redaction}
    \vspace{-15pt}
\end{wrapfigure}

Quite a few official government guidelines on the review of sensitive documents mention Adobe Acrobat as a go to tool for redacting text \autocite{thenationalarchivesRedactionToolkitPaper2016}.
Sometimes, the guidelines \textit{are} Adobe's guidelines for redaction \autocite{scottishgovernmentRedactingInformation2019}.

Adobe Acrobat is a well known PDF toolkit that notably enables sensitivity redaction of documents.
The principle is simple: select text, click redact and browse a nested context menu to select an exemption (Figure~\ref{fig:adobe-redaction}).
This can be repetitive as their does not seem to be a way to redact multiple identical instances of the same piece of text at once.

When other PDF editors implement document redaction, they closely mimic this workflow.
For instance, Foxit PhantomPDF is another well known PDF editor that implements a strikingly similar context menu (Appendix~\ref{fig:foxit-menu}) also allows for document wide redaction of a text selection (Appendix~\ref{fig:foxit-redact-all}), something which Adobe implement quite poorly (Appendix~\ref{fig:adobe-redact-all}).

\subsection{Dedicated document redaction tools}

\begin{wrapfigure}{l}{0.4\textwidth}
    \includegraphics[width=\linewidth]{images/related_products/eredact_dropdown.png}
    \caption{e-Redact exemption selection}\label{fig:eredact-dropdown}
    \vspace{-15pt}
\end{wrapfigure}

There are a few tools built specifically for redacting sensitive information,for example \textcite{ERedact} is a Microsoft Office plugin for redacting sensitive information from documents.
\textcite{ERedact} is a purely manual tool, providing no ``aid'' to sensitivity redaction, contrarily to what we are trying to build, it does however provide insights, notably through its User Interface (UI).
It's context menu is even more deeply nested than Adobe Reader's and contains exemptions from many legislations (Figure~\ref{fig:eredact-dropdown}).
We want to avoid such a deep nesting of regularly used actions, for example in this case, it seems like a reviewer would rarely redact a document for US and UK FOIA at the same time.
A solution to this would be to have a setting to select the legal ``framework'' within a separate menu which would reduce deep menu nesting.
e-Redact also adds the ability to collaborate with multiple people on document redactions, but beyond this, it is very similar to PDF Editors and their built-in functionalities.

A lot of the PDF editors' redaction functionality also requires one to traverse the screen to select the type of redaction.
Others such as \textcite{RedactedAIRemovea} use a tooltip above the text selection (albeit not consistently). The menu appears next to the selected text offering actions to take onto a text selection which avoids having to cross long screen distances (Figure~\ref{fig:redactedai}).
A downside of their implementation of a ``popup menu'' is that it takes considerable space, while it does offer quite a few useful actions, its ``remove'' and ``unredact this'' button are quite large.

In fact, \textcite{RedactedAIRemovea} is a similar concept to what we are trying to achieve: it is a web application for redacting sensitive documents. Its flagship feature uses Natural Language Processing (NLP) methods (namely: entity extraction) to provide an aid to document reviewers (Figure~\ref{fig:redactedai}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{images/related_products/redactedai.png}
    \caption{Redacted.ai document view}\label{fig:redactedai}
\end{figure}

%==================================================================================================================================
\chapter{Requirements}

From this overview of literature and related products, we devise a list of functional and non-functional requirements for our application as well as a set of user stories.
We then illustrate these requirements in a set of wireframe diagrams which allow us to assert their usefulness and to verify we did not skim over important ones.

\section{User Stories}

\paragraph*{File upload, text extraction and Machine Learning model}\mbox{}\\
As a redactor I want my file uploaded for redaction to be pre-processed and displayed with redaction aids so that I can redact documents faster.

\paragraph*{Machine learning model explainer}\mbox{}\\
As a redactor, I would like to understand the reasons for the sensitivity classification suggestions so that I can decide on their relevance.

\paragraph*{Active Learning from user redactions to model}\mbox{}\\
As a redactor, I would like the suggested redactions and classification to improve as I redact documents.

\paragraph*{Redaction assistance}\mbox{}\\
As a redactor, I expect some of my redactions to be reflected across an entire document so that I don’t have to go over them again.

\paragraph*{Web application}\mbox{}\\
As a redactor, I would like to redact on my MacOS, Windows or Linux based computer so that I don’t have to worry about the device I am using.

\paragraph*{Save and Edit redactions}\mbox{}\\
As a redactor, I want to add and remove redactions so that I can go back and edit my work.

\paragraph*{API specification}\mbox{}\\
As a developer I want to standardize API calls so that others can easily build upon or improve the final application

\section{Scope and non-functional definition}

We should reiterate that our objective is not to have a ``production ready'' deployable application.
Rather the emphasis is on producing a proof of concept application to experiment with sensitive document visualizations.
As such, we de-prioritized features like reviewer authentication and having a deployable application.

We aim to bring a ``modern looking'' interface, tools like PDF editors or the Office plugin we saw provide an ageing interface often with excess complexity.
We think that an approach with correctly applied modern design guidelines such as Google's \textcite{MaterialDesign} improves user experience, both in terms of aesthetics and usability.

Originally, we set out to focus on both document set visualization as well as single document visualization, ultimately however the focus shifted a lot towards the document viewer.
We also skipped over some requirements which were later deemed non essential during rescopes of the project (e.g. fulltext document search)

\section{MoSCoW Functional Requirements}

% TODO reformulate this a bit
\begin{adjustbox}{width=\textwidth,center}
    \begin{minipage}[t]{.5\linewidth}
        \centerline{\textbf{Must Have}}
        \begin{enumerate}[noitemsep,nolistsep,label=\textbf{M\arabic*}]
            \item Document Set creation and deletion
            \item Text document creation and deletion within set
            \item Predicted binary document sensitivity classification
            \item Explanation for classifications
            \item User classification of a document
            \item Document set statistics (number of sensitive documents etc\ldots)
            \item Order documents by sensitivity
            \item Editable user redactions of a document
            \item Redacted document exporting
            \item Documentation for API
        \end{enumerate}
    \end{minipage}
    \hfill
    \noindent
    \begin{minipage}[t]{.5\linewidth}
        \centerline{\textbf{Should Have}}
        \begin{enumerate}[noitemsep,nolistsep,label=\textbf{S\arabic*}]
            \item User redactions helpers (redact all instances of text at document or set level)
            \item Suggest document redactions
            \item Fulltext document search
        \end{enumerate}
        \vspace{0.5cm}
        \centerline{\textbf{Could Have}}
        \begin{enumerate}[noitemsep,nolistsep,label=\textbf{C\arabic*}]
            \item Document Entity Recognition
            \item Text classifier selection
            \item Documentation for API SDK
            \item Documentation for frontend
            \item Reviewer authentication
        \end{enumerate}
    \end{minipage}
\end{adjustbox}


\begin{adjustbox}{width=\textwidth,center}
    \begin{minipage}[t]{\linewidth}
        \centerline{\textbf{Would Have}}
        \begin{enumerate}[noitemsep,nolistsep,label=\textbf{W\arabic*}]
            \item Handle more than text files (PDF, Word\ldots)
            \item ``Active Learning'': user redactions improve future predictions
            \item Deployed ``production'' web application
        \end{enumerate}
    \end{minipage}
\end{adjustbox}

\section{Wireframing}

Once we defined the scope of our project, we set out to draw some wireframe diagrams in order to better understand the layout of the application, the use of screen space, as well as how features tie in together and whether we were missing any.
Our approach was to first define the three main ``views'' of our application:

\begin{itemize}[noitemsep,nolistsep]
    \item Homepage (Appendix~\ref{fig:home-wireframe})
    \item Document set view (i.e. documents within the set) (Appendix~\ref{fig:set-view-wireframe})
    \item Single document view (Figure~\ref{fig:document-wireframe})
\end{itemize}

In Figure~\ref{fig:wireframes} we sketch the various aspects of our vision for the single document view page.

Figure~\ref{fig:document-wireframe} is an overview of what the page could look like.
The layout is constructed of three columns, with controls gathered within a menu on the left hand side while centering the document on the page with insights about document sensitivity on the right hand side.

Figures~\ref{fig:tooltip-wireframe}~and~\ref{fig:expanded-tooltip-wireframe} detail the tooltip or ``popover'' style menu that would appear above the text with actions to perform on the selection.
We illustrate features like a ``redact this instance of text'', ``redact all instances of this text in the document'', ``remove the redaction'' or ``comment''.
The latter, is portrayed more in depth as an expanded menu in Figure~\ref{fig:expanded-tooltip-wireframe} showing the ability to select the chosen exemption as well as commenting on why this section of text was redacted.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=0.5\linewidth]{images/wireframes/doc_view.jpg}
        \caption{Document View Wireframe}\label{fig:document-wireframe}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \begin{subfigure}[b]{0.59\linewidth}
            \includegraphics[width=\linewidth]{images/wireframes/tooltip.jpg}
            \caption{minimized tooltip wireframe}\label{fig:tooltip-wireframe}
        \end{subfigure}
        \begin{subfigure}[b]{0.2\linewidth}
            \includegraphics[width=\linewidth]{images/wireframes/tooltip_comment.jpg}
            \caption{Expanded tooltip wireframe}\label{fig:expanded-tooltip-wireframe}
        \end{subfigure}
    \end{subfigure}
    \caption{Selection of some of the Wireframes drawn up during the design process}\label{fig:wireframes}

\end{figure}


%==================================================================================================================================
\chapter{Design}

By reviewing the literature on TAR and document visualization along with an analysis of similar products, we have extracted key requirements both functional and non-functional for our application.
We have pictured how they can be assembled into a full fledged application by wireframing a UI design.
We now elaborate on our application's high level structure as well as the different technologies we opted to assemble to devise our project.

\section{Application structure}

In Figure~\ref{fig:design_diagram} we illustrate the different services of our application as well as the flow of information during a sensitivity review workflow.
The user interacts with the frontend to upload documents which are then sent to the API which classifies them and calculates the explanations.
The API then stores the document in the backend and sends the classification and explanations to the frontend which will then display them to the reviewer.
Based on this, the user will redact parts of the document, these redactions are then sent back to the API which will store them in the database.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/design_diagram.pdf}
    \caption{Application Structure and information flow}\label{fig:design_diagram}
\end{figure}

\section{Machine Learning Model and explanations}

% TODO this sounds too much like a list
% TODO justify choice of ML library
The core of our application is the Machine Learning (ML) model.
To build it we used the Scikit-Learn library \autocite{pedregosaScikitlearnMachineLearning2011} for predicting the sensitivity of documents to meet requirement \textbf{M3}, \textbf{M7} and \textbf{M6}.
The library has a very well implemented and documented intuitive APIs for many ML classifiers (SVM, Random Forest \ldots) as well as text processing (TF-IDF, Bag of Words \ldots) evaluation and tuning (Grid Search \ldots) techniques.
However, it sometimes fails to place performance at its core, which led us to replace its SVC implementation with a drop-in parallelized replacement ``ThunderSVM'' which can operate on multicore CPUs or CUDA enabled GPUs \autocite{wenThunderSVMFastSVM2018}.

We satisfy our requirement \textbf{M4} for explanations and more generally better understanding of our classifier's predictions using techniques implemented by the aforementioned \textcite{ribeiroMarcotcrLime2020} library \autocite{ribeiroWhyShouldTrust2016} which provides per-feature weights explaining the feature's contribution to the final classification.
We experimented with the previously mentioned \textcite{lundbergSlundbergShap2020} library to obtain more relevant libraries than LIME, but settled on LIME due to compute time complexity as well as implementation issues.

\section{OpenAPI specification and generator}

``The OpenAPI Specification is a community-driven open specification within the OpenAPI Initiative, a Linux Foundation Collaborative Project.'' \autocite{OAIOpenAPISpecification2020}.
% TODO elaborate a bit more on this
It is an increasingly popular framework for formally defining a REST API contract.
Its precision and specificity is such that it has allowed powerful tools to expand it.
Notably, the OpenAPI generator \autocite{OpenAPIToolsOpenapigenerator2020} which, given an OpenAPI specification, allows for the \textit{automatic} generation of API server code in more than 40 languages and API client code in more than 50.
Hence, we chose to write our API specification in OpenAPI format \textit{before} developing the API.
This allowed us to generate boilerplate API server and client code with documentation simply given the precise documentation thus fulfilling requirements \textbf{C3} and \textbf{M10}.

\section{API Server}

Most of our data science tools are written in Python which is why we selected it to write our API server.
Furthermore, the language's broad library selection makes development easier, notably for implementing requirements such as \textbf{W1} and \textbf{C5}
The question of the choice of framework, remained, specifically since the OpenAPI generator allows for the generation of Python Server code with different frameworks.

We chose the implementation that makes use of an OpenAPI wrapper around Flask called Connexion which handles ``the mapping from the specification to the code.
This incentivizes you to write the specification'' \autocite{ZalandoConnexion2020}.
Furthermore, Flask is a Python API framework that is extensible with libraries notably implementing file upload size limits for requirement \textbf{W2}. Furthermore it is widely used by major technology companies like Reddit or Netflix \autocite{WhyDevelopersFlask}, a testament to its robustness and level of support which streamlines the implementation of requirement \textbf{M9}.

Furthermore, the Connexion+Flask implements the Oauth2 Authentication specification for API endpoints, a state of the art authentication framework that allows us to securely authenticate reviewers for requirements \textbf{C5} \autocite{jonesOAuthAuthorizationFramework2012}.

\section{Frontend}

We chose to implement a web application as the frontend for our application.
This allows for compatibility with many devices: generally anything that can run a reasonably modern web browser can use a web application without worrying about Operating System choice or GUI library compatibility.
We identified early on the need for a ``dynamic'' application meaning that in order to implement a web application, the codebase would be prominently Javascript (JS).

There are a number of modern frameworks that facilitate JS web application development and avoid having to write plain JS.
Their ``component'' approach to UI elements allows for the development of standalone modules that can be combined into an application, streamlining development by relying on high quality component libraries such as \textcite{Materialui2020} or visualization plotting libraries like \textcite{Recharts2020}.
There are three major web JS frameworks: \textcite{Angular2020,FacebookReact2020,VuejsVue2020}.

Angular is a web framework developed by Google, development uses TypeScript (TS) which is a type language that can transcompile into Javascript.
It has a strict separation of styles (CSS), markup (HTML) and function (TS) and is generally much more ``opinionated'' on application structure and development.
These ``restrictions'' can, at the scale of large projects become standards enabling more streamlined collaboration, at our scale however (one developer), we viewed them as more of a hindrance than an enablement especially since our scope is that of a \textit{Proof of Concept} more than a final large scale project \autocite{wohlgethanSupportingWebDevelopmentDecisions2018}.

VueJS holds similar ``opinions'' on project structure but encapsulates all aspects of a component into a \lstinline{.vue} file albeit while being more ``liberal'' than Angular (by externalizing State management and routing for example) \autocite{wohlgethanSupportingWebDevelopmentDecisions2018}.
Furthermore, like Angular, VueJS introduces a special syntax for enhancing the templates with \lstinline{for} loops and \lstinline{if} statements.

Perhaps the ``most flexible'' framework is ReactJS which we chose as our frontend framework.
ReactJS uses \lstinline{.jsx} files which allow for the integration of HTML and CSS within JS, which avoids the custom syntax used by Angular and VueJS and allows for the versatile use of JS for introducing logic into templates.
Furthermore ReactJS is an Open Source project developed by Facebook, this backing along with its flexibility has given birth to a large ecosystem of both development support and advanced component libraries \autocite{wohlgethanSupportingWebDevelopmentDecisions2018}.
It does however introduce more complex state management due to its ``one-way data binding'': information flows from parent to child component but the reverse operation needs to be done through callbacks or using a state management libraries like \textcite{Redux2020}.
These problems mostly become prominent on larger scale projects, hence we opted for the flexibility of ReactJS for a quickly starting development, albeit encountering some of these state management issues in the latter stages of the project.
% TODO refer back to proof of concept scope of project to justify this

\section{Packaging application components}

We chose to package all these services as \textcite{Docker2020} containers.
Docker is a high level container-based virtualization tool.
A docker container performs better than a Virtual Machine (VM) as it runs on the host's kernel while also providing a good level of isolation from the host through namespaces and cgroups.
Most importantly, the docker ecosystem is much evolved than alternative containerization technologies like \textcite{Lxc2020} thanks notably to the \textcite{DockerHub} with many base images, but also a more widespread use, documentation and support as well as powerful orchestration tools for large scale deployments like \textcite{Kubernetes2020}.

In our case however, we do not go as far as deploying the application, as such, we have no need for a powerful (and complex) orchestrator.
Rather we opt to ``orchestrate'' our docker containers using \textcite{DockerCompose2020}, a simpler python program to define a \verb|docker-compose.yml| file containing port mappings, container, network and volume definitions for a docker-compose ``stack'' of containers thus combining multiple services into a usable application.

The next section details some of the key pieces of code from these services as well as how they interact together.


%==================================================================================================================================
\chapter{Implementation}

\begin{wrapfigure}{r}{0.356\textwidth}
    \includegraphics[width=\linewidth]{figures/tech_stack_no_background.pdf}
    \caption{Harpocrates tech stack}\label{fig:tech_stack}
    \vspace{-50pt}
\end{wrapfigure}

Harpocrates is a hellenized interpretation of the child appearance of the Egyptian deity Horus \autocite{mattheyChutSigneHarpocrate2011}.
In Greek mythology, Harpocrates is the God of silence and keeper of secrets, a fitting name for our project.
Figure~\ref{fig:tech_stack} illustrates the three containers that make up ``Harpocrates''.

The first is an \textcite{NGINX2020} web server containers that contains and serves the compiled source code for our ReactJS frontend.
It interacts with a python container which run the Flask API server, this image is based off \textcite{GoogleContainerToolsDistroless2020}'s smaller and more secure ``distroless'' python 3 image \autocite{mooreDistrolessDockerContainerizing2017}.
Lastly, the API uses the official \textcite{MongoDB2020} container as a NoSQL database for storing documents, redactions and classifications.

\section{Redacting sensitive sections}



In order to select sensitive sections of a document for redaction, we initially embarked on creating our own text selection ``popover'' menu for our ReactJS frontend.

Our base was \textcite{krispel-samselJuliankrispelReacttextselectionpopover2020}, a text selection popover library for ReactJS.
Figure~\ref{fig:popover_1} shows our resulting initial popover menu implementation.
The next step was to extract the document-wide offset for the selected text.
We managed to do this and implement a redaction functionality to black out selected text.
\begin{wrapfigure}{l}{0.45\textwidth}
    \includegraphics[width=\linewidth]{images/popover_menu_1.jpg}
    \caption{Initial sensitive section selection popover menu}\label{fig:popover_1}
    \vspace{-10pt}
\end{wrapfigure}
However, we could not find a way to do this without inserting a Document Object Model (DOM) node in the middle of the document text.
This results in the character offset for the next selected piece of text to be calculated from the previous DOM node as opposed to the start of the document.
We implemented a fix by traversing the DOM tree to sum each sibling node's character offsets and recalculating the document wide character offset.
However, the final result however did not handle edge cases like overlapping text selections, instead of trying to fix every edge case, we moved on to another solution.

We fell back to \textcite{camachoMcamacReacttextannotate2020} a ReactJS component for labelling sections of text.
While not perfect (e.g. unable to customize style of selected text, no popover menu) our final solution using this library saved us time while allowing us to have basic document redaction up and running.
In the future, this library could be extended to allow for the implementation of these missing features.

\section{OpenAPI definitions}

To bind our application together, we define OpenAPI endpoints and models, below is an example of the \verb|predictedClassification| model along with its \verb|GET| endpoint definition.
Listing~\ref{listing:predictedClassification_yml} and Listing~\ref{listing:getPredictedClassification_yml} provide an example of an OpenAPI definition of the model of a response along with an accompanying endpoint definition.

\begin{listing}[H]
    \inputminted{yaml}{code/predictedClassification.yml}
    \caption{Defining a predictedClassification response object in OpenAPI}\label{listing:predictedClassification_yml}
\end{listing}

One thing to note is that OpenAPI has variable types which can be defined very precisely, allowing for strict type and value checking of variables at both the server and client levels (Listing~\ref{listing:predictedClassification_yml}, line 11-15).

Another interesting aspect of the specification language is that it allows us to define an \verb|object| and extend it with another other \verb|object|.
This object oriented approach allows us to avoid code repetition but also automatically define classes through the code generator.

\begin{longlisting}
    \inputminted{yaml}{code/getPredictedClassification.yml}
    \caption{Defining a GET endpoint in OpenAPI}\label{listing:getPredictedClassification_yml}
\end{longlisting}

\section{Machine Learning Pipeline}

The core of Harpocrates: its document classifier consists of a number of steps implemented with multiple python libraries assembled in a scikit-learn \verb|Pipeline| (Listing~\ref{listing:ml_pipeline}).

The first step consist in a TF-IDF transformer to convert the document's text to a numeric vector.
We optimized our vectorizer's parameters by performing a grid search with five fold cross-validation to avoid overfitting, the optimal parameter space depends on the use case.

We then resample our highly unbalanced training data with the \textcite{ScikitlearncontribImbalancedlearn2020} library from \textcite{lemaitreImbalancedlearnPythonToolbox2017}.
It implements common resampling techniques \autocite{lemaitreImbalancedlearnPythonToolbox2017}, we use a combination over under and over sampling depending on the use case.
In this example (Listing~\ref{listing:ml_pipeline}), we make use of Synthetic Minority Oversampling TEchnique (SMOTE) oversampling combined with Tomek Link removal undersampling \autocite{batistaStudyBehaviorSeveral2004}.

We use a Support Vector Classifier (SVC) which has performed best for sensitive document classification in past research \autocite{mcdonaldClassifierDigitalSensitivity2014,mcdonaldStudySVMKernel2017}.
We replace scikit-learn's SVC for a parallelized implementation for multicore CPUs and CUDA enabled GPU in order to improve performance \autocite{wenThunderSVMFastSVM2018}.
We also perform a grid search with five fold cross-validation to tune the parameters of the SVC, once again, the optimal parameter space depends on the application.

\begin{listing}[H]
    \inputminted{python}{code/ml_pipeline.py}
    \caption{Machine Learning classification Pipeline}\label{listing:ml_pipeline}
\end{listing}

\section{Classification Explanations}

As mentioned, we do not simply want to give reviewers a predicted classification but also accompany it with a simple explanation.

The two main libraries we tried ML explanations were LIME and SHAP, while the latter has preferable results (Section~\ref{section:lit_review}) it is much slower than LIME for non tree-based classifiers (like SVM).
Future research might want to investigate boosted trees \autocite{chenXGBoostScalableTree2016} and their performance for sensitivity classification in order to implement SHAP explanations with reasonable performance.

Listing~\ref{listing:ml_explanations} shows the important code used for calculating our LIME explanations.
A key parameter is the \verb|num_features|, it is the $k$ value in the LIME algorithm mentioned previously: compute time increases as the number of calculated explanations increase.
The returned value is a LIME \verb|Explanation| object wich we later exploit as a dictionary mapping features to their weight.

\begin{listing}[H]
    \inputminted{python}{code/explanations.py}
    \caption{Machine Learning classification explanations}\label{listing:ml_explanations}
\end{listing}

\section{ReactJS components}

\begin{wrapfigure}{r}{0.45\textwidth}
    \includegraphics[width=\linewidth]{figures/sensitivity_bar.pdf}
    \caption{Rendered sensitivity bar}\label{fig:sensitivity_bar_preview}
    \vspace{-10pt}
\end{wrapfigure}

ReactJS' latest implementations take a functional approach to web component definition: Listing~\ref{listing:sensitivity_bar} demonstrates this with one of the simpler components of our application (Figure~\ref{fig:sensitivity_bar_preview}).

One of the features of ReactJS is its ``one-way data binding'' that is data is passed from parent to child component using a \verb|props| dictionary.
In Listing~\ref{listing:sensitivity_bar}, we pass a document's classification to the \verb|SensitivityBar| component to change its appearance based on its sensitivity value.
This one way data binding simplifies the flow of information thus making them easier to debug.


\begin{listing}[H]
    \inputminted{jsx}{code/documentSensitivityBar.jsx}
    \caption{Document sensitivity bar}\label{listing:sensitivity_bar}
\end{listing}

\section{API calls from ReactJS components}

As mentioned, the OpenAPI specification allows us to generate a finished JS API client.
Provided our server implementation matches our specification, there is no need to modify the generated API client and it works ``out of the box''.

Listing~\ref{listing:newSetForm} illustrates this API client in use.
On line 13 we instantiate our client which we then use to post a new set to the API in the event handler function (Line 15) for our submit form.
We assign our event handler as a callback to the set creation form on Line 32.

In Listing~\ref{listing:newSetForm} we also see an example of the state management in ReactJS.
Upon instantiating \verb|newSetName| with a default value on line 10, we also receive a setter function to modify the component's newly created state.
When the value within the \verb|TextField| component (Line 34) changes, this setter function is used to modify the component's state with the new value for the set name.

\begin{listing}[H]
    \inputminted{jsx}{code/newSetForm.jsx}
    \caption{New document set form}\label{listing:newSetForm}
\end{listing}

\section{Version-Control and Continuous Integration}

Throughout the project, we tried various repository organizations, hosted git platforms and Continuous Integration (CI) tools.

\subsection{Mono-repository}

Initially, we setup our project as multiple repositories within a group on \href{https://gitlab.com/harpocrates-app}{gitlab.com}, thus holding the frontend, server, api client and specification in their own repositories.
However, the repository management overhead introduced was too complex. A change in the specification would imply a commit in all 3 other repositories.
To counter this, we shifted to a project ``monorepository'' which provide ``better managing of cross-project changes, easy refactoring, simplified organization'' \autocite[1]{britoMonoreposMultivocalLiterature2018}.

\subsection{Continuous Integration}

We chose \href{https://gitlab.com/}{gitlab.com} because of its powerful and well documented Continuous Integration (CI) capabilities.
We made use of these features in order to automatically build and test the the OpenAPI specification, docker containers and the documentation for each commit we setup a CI Pipeline (Figure~\ref{fig:ci}) .

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/ci.png}
    \caption{CI Pipeline}\label{fig:ci}
\end{figure}

In a first \verb|test| stage we test our API specification, the accompanying API client and run a code quality check using \textcite{Codeclimate2020} to check for common programming mistakes.

In the \verb|build-documentation| stage build the documentation for our API, its client and the general application as static web pages that are later auto published at \href{https://harpocrates-app.gitlab.io/harpocrates/}{harpocrates-app.gitlab.io/harpocrates} with Gitlab Pages in the \verb|deploy| stage.

Most importantly, the \verb|build| stage builds the docker containers for our frontend and API.
Once the images are built, they are tagged with either the branch name, or the commit hash and published on a \href{https://gitlab.com/harpocrates-app/harpocrates/container_registry}{docker container registry hosted on gitlab.com}.

\subsection{Github bots and integrations}

We later moved our main repository to \href{https://github.com/guillaumedsde/Harpocrates}{github.com}, to make use of multiple helper bots on Github while keeping a mirror of the repository on Gitlab to avoid redifining our CI Pipeline.

During development, multiple of our libraries received updates.
To avoid having to hunt down and keep track of updates, we set up \textcite{Dependabot2020}, to automatically create Pull Requests (PR) on our repository for dependency updates.

We also kept track of our tests' code coverage for each Pull Request using \textcite{Codecov2020}.
It is another Github bot that calculates code coverage for every commit and posts code coverage changes in PRs.

%==================================================================================================================================
\chapter{Evaluation}

\section{Code Testing}

The first kind of evaluation we carried out was by implementing testing on every commit for our application.

We introduced a step in our CI pipeline for validation the OpenAPI specification using a validator for the specification file \autocite{IBMOpenapivalidator2020}.
This allowed to make sure there was no regression in the quality of the specification, warning about missing examples and descriptions.
Lastly, it also checks for outright incorrect specifications, verifying that no required fields are missing and asserting the correctness of the YAML syntax.

Futhermore, the CI docker container builds provide intrinsic build tests, they fail if all dependencies are not met for both our Python API and ReactJS frontend.
This is particularly true of our frontend which uses \textcite{ParcelbundlerParcel2020} which verifies the JS syntax and asserts dependencies are met before bundling the final application.

They are many tools which implement automated tests given only an OpenAPI specification.
In our case, \textcite{OpenAPIToolsOpenapigenerator2020} generated a test suite for our JS API client which we implemented in our CI pipeline along with code coverage evaluation using \textcite{Codecov2020}.

We lacked time to implement similar specification based testing on our API server code, specifically: the API endpoints.
One such tool we wished to implement is \textcite{KiwicomSchemathesis2020} a tool targeted at any API but particularly well integrated with Flask.

\section{Expert Panel Review}

The second type of evaluation was to get the application reviewed for feedback and suggestions with the help on an expert panel.
We are very grateful for the contribution of this panel constituted of software developers working to implement TASR solutions.

We demoed the application to the panel, explaning the various parts of the UI, some of its functioning as well as simulating a workflow with simultaneous ``think aloud'' feedback.

We got positive feedback on the ``workflow oriented'' layout of the application as well as its complete yet extendable use of screen space.
While controlling the number of explanation features was appreciated, a note was made on the lack of ability to control the infomration density within the document (e.g. viewing two pages at onces).
A fix for this would be to have a layout more adaptable to various screen sizes. %TODO justify this as out of scope?

A remark was also made on the usefulness of the classification explanation words, noting that it was hard to relate them to sensitivity review.
This is a result of our use of an unrelated dummy document to not reveal part of our document collection, reflecting on this, we should have used a more appropriate dummy document.
Another related suggestion was to perhaps target classification at specific sections of the FOIA instead of a broad ``sensitivity'' classifier which might, in turn yield more relevant explanations.

An important point was also brought up about the conservation of document structure and the redaction of non textual elements in a document.
Our project's scope focused on plaintext documents, this implies a ``loss'' of non-textual aspects of a document.
Future development could focus on handling PDF files, a widely used, cross-platform document format for preserving non-textual document elements.


%TODO mabye?    little information outside of document view (e.g. document set view)
%                   preview some of the redactions of the document?
%                   more details without entering the document view
%               Would be nice to be able to hide or remove explanations
%               Experimenting with colors: why not, but be careful of color blindness 


\section{User Study}

Lastly, we prepared a user study in order to evaluate the effectiveness of our application in helping reviewers conduct a sensitivity review.
Through this study, we sought to answer a handful of research questions:

\begin{enumerate}[label=\textbf{RQ\arabic*}]
    \item Does our visualization of predicted document sensitivity and explanation features help reviewers conduct a sensitivity review faster?
    \item Does it improve the accuracy of the sensitivity review process?
    \item Does it improve reviewers' confidence in the completeness of their sensitivity review?
\end{enumerate}

\subsection{Dataset and classifier experimentations}

Before conducting the study, we aimed to improve the classifier to obtain sufficient performance so that it could be as helpful as possible to participants.
The document collection we had access to is a set of 3801 Government documents relating to International Activities.
The ground truth for document sensitivity was established by government sensitivity reviewers with respect to S40 (Personal Information) and S27 (International Relations) of the FOIA.
This appraisal resulted in 502 (13\%) documents deemed sensitive to either S40, S27 or both with 3299 non sensitive documents (87\%).

Due to our lack of access to expert document reviewers, we sought to conduct our study with students either from an academic Politics background or with International Relations awareness.
We focused on identifying ``Personal Information'' exceptions to the FOIA which we deem more ``attainable'' given our non-expert test subjects.
Moreover, following the suggestions from our expert panel review and our supervisors, we expected to obtain better performance by targeting our classifier at a specific sensitivity rather than on our entire range of sensitivities (S27 and S40 in our case).

In particular, our dataset contained 289 documents with S40 exemptions, we split the collection in order to obtain a stratified sample ``test set'' 20\% the size of the entire collection.
We vectorized the remaining 80\% ``train set'' using Scikit-Learn's \verb|TfidfVectorizer|.
We then resampled the resulting vectors to achieve better class balance with imblearn's \verb|SMOTEENN|, a combination of majority class downsampling and minority class upsampling \autocite{lemaitreImbalancedlearnPythonToolbox2017}.
We used the resulting collection of resampled vectors to train a SVC for binary classification of documents: whether or not they contain sensitive information according to S40.
Table \ref{tab:clf_perf} shows the results we obtained while trying to find a classifier with satisfactory performance for our user study.

\begin{table}[H]
    \begin{adjustbox}{width=\textwidth,center}
        \begin{tabular}{l|llllll}
            Classification pipeline                  & Tuned For & FOIA   & BAC    & \(F_{2}\) & Precision & Recall \\ \hline
            CountVectorizer, RandomUnderSampler, SVC &           & 27, 40 & 0.4993 & 0.0847    & 0.0262    & 0.1920 \\
            CountVectorizer, SMOTE, SVC              &           & 27, 40 & 0.4856 & 0.1549    & 0.1151    & 0.1829 \\
            TF-IDF, SMOTEENN, SVC                    & BAC       & 40     & 0.5576 & 0.1182    & 0.0659    & 0.1911 \\
            TF-IDF, SMOTEENN, SVC                    & \(F_{2}\) & 40     & 0.5482 & 0.1331    & 0.0385    & 0.4047 \\
            TF-IDF, RandomUnderSampler, SVC          & BAC       & 27, 40 & 0.5526 & 0.3593    & 0.1669    & 0.5454 \\
            TF-IDF, RandomUnderSampler, SVC          & \(F_{2}\) & 27, 40 & 0.5079 & 0.4359    & 0.1340    & 0.9980 \\
        \end{tabular}
    \end{adjustbox}
    \caption{Best performing classifiers using 5-fold cross validation targeting different metrics and sensitive classifications}\label{tab:clf_perf}
    \vspace{-10pt}
\end{table}

Table~\ref{tab:clf_perf} confirms our assumptions to some extent.
When targetted at FOIA S40, the BAC of our TF-IDF, SMOTEENN with SVC pipeline exceeds that of the others we tested.
In terms of \(F_{2}\) (favouring recall) however, we observe that our TF-IDF, RandomUnderSampler with SVC pipeline performs best when trained for both S27 and S40 and tuned for \(F_{2}\).
This is due to its particularly high recall (with low precision).

\subsection{Experimental Setup}

We opted to select our TF-IDF, SMOTEENN with SVC pipeline targeted at FOIA S40 for our user study.
This is in part because of our participants lack of experience but also based on our unverified intuition that explanations from the targeted classifier would be more useful than from the more general S27 and S40 classifier.

Due to time constraints, we only sampled documents to review from the test set that were under 2000 characters.
We then selected both sensitive and non-sensitive documents in order to represent every category of the confusion matrix for the classifier as illustrated in Table~\ref{tab:confusion-matrix-selection}.

As an aside note: we have left in the repository the scripts we used for the document selection and general experimental setup with fixed random number generator seeds so that anyone with the same dataset can reproduce this experiment with the same documents, classifications and explanations.

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{l|ll}
                                & Actually non-sensitive & Actually Sensitive \\ \hline
        Predicted non-sensitive & 3                      & 1                  \\
        Predicted Sensitive     & 1                      & 1
    \end{tabular}
    \caption{Count of selected documents in the classifier's confusion matrix}\label{tab:confusion-matrix-selection}
    \vspace{-15pt}
\end{table}

We sample test documents accordingly twice: once for each User Interface (UI).
One interface \textit{test mode 1} (Figure~\ref{fig:test-mode-1}) is a simplified version of the final interface containing the classification prediction and the accompanying explanations.
The other, \textit{test mode 2} (Figure~\ref{fig:test-mode-2}) is a stripped down version that only displays the document, and the manual redaction tools (document title and sensitivity type selection).

\begin{figure}[H]
    \centering
    \begin{subfigure}[c]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/ui_test_mode_1.png}
        \caption{Test mode 1}\label{fig:test-mode-1}
    \end{subfigure}
    \begin{subfigure}[c]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/ui_test_mode_2.png}
        \caption{Test mode 2}\label{fig:test-mode-2}
    \end{subfigure}
    \caption{The two test modes used for evaluation}\label{fig:test-modes}
    \vspace{-15pt}
\end{figure}

We asked our participants to review each set of test documents with one of the two interfaces.
The starting interface and the document order within each set was chosen randomly in to minimize the learning effect on the reviewing task.
After each batch of documents, we handed our reviewers sections of a questionnaire (Appendix~\ref{appendix:questionnaire}) to record their impressions about the interface, confidence about their work and time to review each document.
This questionnaire is split into three sections, the first two were an identical based on the System Usability Survey (SUS) format \autocite[Chap.~21]{jordanUsabilityEvaluationIndustry1996} which aimed at evaluating the interface as a whole.
The first two sections were answered after each review batch was completed, then, the last section was handed out.
It seeks to evaluate the usefulness of each invidual component in interface 1.

\subsection{Results}

As mentioned, each user is asked to review one collection of six documents for personal information sensitivities (FOIA Section 40) for each user interface.
The independent variables will a set of two user interfaces: with and without the predicted classification and explanations (more details below) which all users will both use. We will measure multiple dependent variables against our two interfaces (the independant variables):

\begin{itemize}
    \item Firstly, we measure the time to review each document in both collections.
          We normalize document review time with the document's word count in order to account for differing document length, although it should be noted that a simple word count does not necessarily reflect the complexity of a document, especially for sensitivity review.
    \item Secondly we record the accuracy of each reviewer on each interface for all documents.
          We extract two measures from our participants' review batches: the balanced accuracy and \(F_{2}\), measures particularly well suited to evaluating sensitivity review \autocite{mcdonaldStudySVMKernel2017}.
    \item Lastly, we evaluate the confidence of the reviewers after using each interface with a Likert scale in the questionnaires.
\end{itemize}

% t-test for review time (because time is a continuous variable)
Regarding \textbf{RQ1}, we hypothesise that \textit{There is a statistically significant improvement to the document review time with interface 1} (\textbf{H1}) along with a null hypothesis stating that \textit{There is no statistically significant improvement to the document review time with interface 1} (\(\mathbf{H1_{null}}\)).
To evaluate this, we perform a paired sample two-tailed Student's T-Test, pairing review times of a given document on interface 1 and on interface 2.
Since we randomely alternate between both interfaces we are able to pair all review times for a document reviewed on interface 1 with another review time for that same document on interface 2.
Ideally, we expect our p-value to be lower than our critical value at a 95\% confidence level, allowing us to confirm \textbf{H1} for our sample.

In answer to \textbf{RQ2} we suggest that \textit{Interface 1 significantly improves the the accuracy of reviewers} (\textbf{H2}) with our null hypothesis thus being that \textit{Interface 1 does not significantly improve the the accuracy of reviewers} (\(\mathbf{H2_{null}}\)).
To confirm \textbf{H2} or \(\mathbf{H2_{null}}\), we evaluate the significance for both the difference in balanced accuracy and \(F_{2}\) measure at 95\% confidence.
Since Interface 1 essentially contains a predicted sensitivity which might be taken as a guideline by our participants, we assume an unequal variance in distribution of review performance in interface 1 and 2.
Indeed, we expect the accuracy for documents reviewed with interface 1 to be more ``clustered'' together: namely around the classifier's predicted sensitivity.
Once again, we perform a paired sample two-tailed Student's T-Test, pairing performance for a given document on interface 1 with that same metric and document on interface 2.

Lastly, we suppose that \textit{Interface 1 significantly improves the the confidence of participants in the quality of their sensitivity review} (\textbf{H3}) or that \textit{Interface 1 does no significantly improves the the confidence of participants in the quality of their sensitivity review} (\(\mathbf{H3_{null}}\)) in response to \textbf{RQ3}.
Since our sample size is expectedly quite restricted, we prefer Fisher's exact test over a Chi-square test for testing the significance of the difference in confidence between interface 1 and 2 to validate \textbf{H3} or \(\mathbf{H3_{null}}\).
We recode our 5 point Likert scale into three categories: Improved, Unchanged and Worsened Confidence
%
With these categories, we peform a two-tailed Fisher's exact test to evaluate the significance of a difference in confidence between interfaces.
Ideally, we expect our p-value to be inferior than our critical value at a 95\% confidence level, which would confirm \textbf{H3} over \(\mathbf{H3_{null}}\).

\section*{}

Concluding this project in unexpected circunstances has left some things yet to be finished for this user study.
Sensitivity Review being a complex matter, an in person user study is important for being able to explain the task and measure it properly: especially with novice reviewers like our targeted population.
As we have mentioned we have left the tools we develop to conduct the user study, including the questionaire and setup scripts in order to allow for it to be conducted at a later date.
This might also allow more time to recruit more participants than we would have or to recruit expert reviewers.

We can note a couple of points from our one test user evaluation.
There is still some work to be done on the text selection for redactions, notably, their is a noticable delay between selecting text and it actually becoming highlighted du to frontend waiting for confirmation for the backend.
Furthermore, text selection is still sometimes buggy, notably when selecting overlapping sections of particular text.
We also refactored the questionaire following the test user evaluation, notably the SUS section to clarify and reformulate section in order to emphasize that questions are to be answered from a sensitivity reviewer's perspective.

Despite these shortcoming we feel that, especially since this is a proof of concept, sufficiently useful and constructive feedback and evaluation has been obtained from the expert panel's review to consitute a preliminary assessment of Harpocrates.


%==================================================================================================================================
\chapter{Conclusion}

As much research often does: this project has introduced more questions than it has answered.
It has however addressed the core of the initial scope, namely, to develop a proof of concept application for aiding sensitivity review with Machine Learning tools.

\section{Achievements and reflections}

While we have yet to evaluate whether it contributes to actually improving accuracy or review time, we have devised a user study to verify these claims along with the tools necessary to reproduce the a very close experimental setup.

Our final application ``Harpocrates'' allow reviewers to upload documents, and have them displayed within a modern and simple interface for redaction.
This interface allows reviewers to create editable redactions of sections of a document for a particular exemption with the aid of a selectable Machine Learning classifier complemented by an explanation for its classification.
Once a reviewer is done with a document, he can chose to export the original or the redacted plaintext version.
The document set view displays predicted classification for all documents in the set, albeit not sortable by sensitivity.
The final producted is a partly tested fullstack application with advanced documentation for its API and associated client as well as the product as a whole.

The original design and requirements did however evolve throughout the development process.
Having fulltext search as a feature was not found to directly contribute to the proof of concept and could be implented in a more deployment ready application.
We have not implemented any kind of suggested redaction, something which could perhaps be implemented with entity recognition (also unimplemented) similarly to \textcite{RedactedAIRemovea}.

Learning and using ReactJS has left us partly satisfied and partly

\section{Future work}

Our application does however remain a proof of concept, as such, it is not fully deployable, in part due to its lack of more testing as well as a reviewer authentication scheme.
This can be implemented through OpenAPI support for Oauth2 authentication.

Classifiying documents along with calculating is computationally expensive and thus, time consuming.
Harpocrates needs some sort of computationally expensive task scheduling and distribution like \textcite{Celery2020} as well as a querying mechanism for diplaying document processing progress to the final user.

Furthermore, at scale, such a system would require distributed computing capabilities with a load balancing reverse proxy.
When reaching these scales, previously mentionned orchestration tools like \textcite{Kubernetes2020} really shine for painlessly deploying redudant, distributed and load balanced applications.

A deployable version of Harpocrates would also probably need to be able to process common document formats like Word, PDF\dots %
Displaying them in a web browser along with the ability to redact them is complex but not unfeasible with open source solutions like \textcite{tyurinAgentcooperReactpdfhighlighter2020} or more advanced multi-format commercial solutions like \textcite{ReactPDFLibrary2020}.
To us, it seems like the best approach would be to convert files to PDF on upload and thus focus on developing PDF annotation and export tools.

As we mentioned SHAP explanations are more ideal but more computationally expensive than LIME's.
Proper task scheduling like we mentionned could alleviate some of this issue.
It might also be worth investigating tree based ML classifiers which perform better with SHAP in order to utilize SHAP's more accurate and complete explanations.

The ML classifier could also be furthered: we focused on visualizing its output but there so much more to be done to improve it.
The literature is abundant with techniques which can be used to improve our ``simple'' text classifiers: Part of Speech Tagging, Word Embeddings, Entity Recognition, Semantic Features \dots %
Moreover, all these features introduce a wide range of challenges for visualization, especially those, like word embeddings, which imply illustrating the relationship between terms.

Conducting the user study described previously would also be enlightening for understanding how and if tools like Harpocrates actually improve the sensitivity review process.

% link back to requirements,  how have they been met?
% reflections
% future work

%==================================================================================================================================
%
% 
%==================================================================================================================================
%  APPENDICES  

\begin{appendices}
    % \settocdepth{chapter}
    \chapter{PDF Editors}
    \section{PhantomPDF context menu}\label{fig:foxit-menu}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\linewidth]{images/related_products/foxit_redaction.png}
    \end{figure}
    \section{PhantomPDF redact all}\label{fig:foxit-redact-all}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\linewidth]{images/related_products/foxit_redact_all.png}
    \end{figure}
    \section{Adobe Acrobat redact all}\label{fig:adobe-redact-all}
    \begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{images/related_products/adobe_redact_all.png}
    \end{figure}
    \chapter{Wireframes}
    \section{Homepage}\label{fig:home-wireframe}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\linewidth]{images/wireframes/home.jpg}
    \end{figure}
    \section{Set View}\label{fig:set-view-wireframe}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.65\linewidth]{images/wireframes/set.jpg}
    \end{figure}
    \section{Set View - opened menu}\label{fig:set-view-opened-menu-wireframe}
    \begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{images/wireframes/set-menu.jpg}
    \end{figure}
    \section{Document View v1}\label{fig:document-view-alternative-wireframe}
    \begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{images/wireframes/page.png}
    \end{figure}
    \section{Document View v2 - closed settings drawer}\label{fig:document view-closed-menu-wireframe}
    \begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{images/wireframes/doc-view.jpg}
    \end{figure}
    \chapter{User evaluation questionnaire}\label{appendix:questionnaire}
    \includepdf[
        pages=-,
        % pagecommand={},
        % width=\linewidth
    ]{figures/questionnaire.pdf}
\end{appendices}
%==================================================================================================================================
%   BIBLIOGRAPHY   


\newpage


\section*{References}

\printbibliography[heading=none]

\vspace*{\fill}
% print license
\doclicenseThis%

\end{document}