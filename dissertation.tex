% !TeX root = ./dissertation.tex
% !TeX spellcheck = en-GB
% REMEMBER: You must not plagiarise anything in your report. Be extremely careful.

\documentclass{l4proj}

    
%
% put any additional packages here
%

\usepackage{float}

\usepackage{wrapfig}


% \setlength{\belowcaptionskip}{-15pt}

\setmainlanguage{english}
\usepackage{csquotes}

% Referencing 
\usepackage[backend=biber,bibstyle=apa,date=year,alldates=year,urldate=short,citestyle=authoryear]{biblatex}
\DeclareLanguageMapping{english}{english-apa}
\AtEveryCite{%
  \clearfield{month}%
  \clearfield{day}%
  \clearfield{labelmonth}%
  \clearfield{labelday}%
  \clearfield{labelendmonth}%
  \clearfield{labelendday}%
}

\bibliography{l4proj.bib}

% break bilbatex reference URL on number
\setcounter{biburlnumpenalty}{9000}

\usepackage{csquotes}

\usepackage{enumitem}

\usepackage{tocvsec2}

\usepackage{epigraph}

% length of epigraph
\setlength\epigraphwidth{12cm}
% remove epigraph bar
\setlength\epigraphrule{0pt}

% \renewcommand{\epigraphsize}{\Huge}



% license
\usepackage[
    type={CC}, 
    modifier={by-nc-sa},
    version={4.0},
]{doclicense}
 


\begin{document}

%==============================================================================
%% METADATA
\title{Why is This Sensitive? \\ Visualising Important Sensitivity Classification Features}
\author{Guillaume de Susanne d'Epinay}
\date{\today}

\maketitle

%==============================================================================
%% ABSTRACT
\begin{abstract}
    Every abstract follows a similar pattern. Motivate; set aims; describe work; explain results.
    \vskip 0.5em
    ``XYZ is bad. This project investigated ABC to determine if it was better.
    ABC used XXX and YYY to implement ZZZ. This is particularly interesting as XXX and YYY have
    never been used together. It was found that
    ABC was 20\% better than XYZ, though it caused rabies in half of subjects.''
\end{abstract}

%==============================================================================

% EDUCATION REUSE CONSENT FORM
% If you consent to your project being shown to future students for educational purposes
% then insert your name and the date below to  sign the education use form that appears in the front of the document. 
% You must explicitly give consent if you wish to do so.
% If you sign, your project may be included in the Hall of Fame if it scores particularly highly.
%
% Please note that you are under no obligation to sign 
% this declaration, but doing so would help future students.
%
%\def\consentname {My Name} % your full name
%\def\consentdate {20 March 2018} % the date you agree
%

\def\consentname{Guillaume de Susanne d'Epinay}
\def\consentdate{\today}

\educationalconsent

\newpage

\vspace*{\fill}

\epigraph{\Large{``Transparency is for those who carry out public duties and exercise public power. Privacy is for everyone else''}}{\fullcite[178]{greenwaldNoPlaceHide2014}}

\vspace*{\fill}

%==============================================================================
\tableofcontents

%==============================================================================
%% Notes on formatting
%==============================================================================
% The first page, abstract and table of contents are numbered using Roman numerals and are not
% included in the page count. 
%
% From now on pages are numbered
% using Arabic numerals. Therefore, immediately after the first call to \chapter we need the call
% \pagenumbering{arabic} and this should be called once only in the document. 
%
% Do not alter the bibliography style.
%
% The first Chapter should then be on page 1. You are allowed 40 pages for a 40 credit project and 30 pages for a 
% 20 credit report. This includes everything numbered in Arabic numerals (excluding front matter) up
% to but excluding the appendices and bibliography.
%
% You must not alter text size (it is currently 10pt) or alter margins or spacing.
%
%
%==================================================================================================================================
%
% IMPORTANT
% The chapter headings here are **suggestions**. You don't have to follow this model if
% it doesn't fit your project. Every project should have an introduction and conclusion,
% however. 
%
%==================================================================================================================================



\chapter{Introduction}

% reset page numbering. Don't remove this!
\pagenumbering{arabic}

\section{The need for Technology Assisted Sensitivity Review}

On June 2nd 2013, in a busy Hongkongese commercial district, refuged in one of the 492 rooms of the Mira Hotel, tens of thousands of neatly organized documents traded hands from a soon to be ex-NSA Employee to two American journalists.
It was not merely the size of the Snowden archive handed over that day that made it ``stunning both in size and scope'', but also that ``it had been produced by virtually every unit and subdivision within the sprawling agency, and [...] closely aligned foreign intelligence agencies'' \autocite[77]{greenwaldNoPlaceHide2014}.
Months later, even after develop�ng indexing and search tools, Greenwald and his team were still pouring over the mountain of documents that had been released.
Despite this, the many hands that worked on the Snowden archive were confronted with a task only a fraction the scale the work of today's sensitivity reviewers.

In the United Kingdom (UK), the \textcite{PublicRecordsAct1958} created what is today known as The National Archives (TNA), a public institution keeper of governmental records that the act stipulates, should be released within thirty years of their creation.
The \textcite{FreedomInformationAct2000} (FOIA) later introduced a public ``right of access'' through requests to information held by public authorities.
Most recently, the \textcite{ConstitutionalReformGovernance2010}.

These documents have to be sensitivity reviewed in order to redact information exempted from disclosure by the FOIA, this includes, amongst other exceptions, personal information or information deemed harmful to international relations.

This sensitivity review process is particularly time consuming because it has to be thorough: everything that is released has to be reviewed.
The scale of this task cannot be understated: in 2014 the total number of documents pending release stood at a staggering 817,615 records across all British government departments \autocite{allanRecordsReview2014,thenationalarchivesRecordTransferReport2014}.
Morever, this number only represents the records that have been accounted for, only within departments that keep track of these figures and must legally transfer their record to TNA.
It is safe to say that Greenwald's team's work on the Snowden archive is dwarfed in scale by the British government's Herculean backlog of records, challenging the established principle of thorough document review.

In order to preserve this paradigm, the magnitude of records calls for digital tools to accelerate the sensitivity review process.
Otherwise, the risk is for Public Institutions to become overwhelmed by their review backlog forcing them to resort to blanket closure of records for extended periods (a century), delegating the handling of sensitive information to the erosion of time and preventing rightful Public access.

\section{Objectives}

Our aim is to contribute to the research into these tools to assist sensitivity reviewers with Technology Assisted Review (TAR).
Notably, we seek to extend the work in this emerging field which has produced Machine Learning classifiers focused on identifying sensitive documents to prioritize and better target reviewing resources and thus process documents faster.
A lot of effort has been dedicated to these Machine Learning algorithms, some work has focused on evaluating their impact on reducing review time, but there is little work on representing these sensitivities and displaying them in a modern user friendly interface.

We want to introduce an approach to visualizing sensitive information within documents based on these Machine Learning classifiers.
Our objective is to design, build and evaluate a proof of concept fullstack application for classifying, visualizing and redacting potentially sensitive documents.
Ideally, we aim to decrease the time reviewers spend on documents and/or allow them to more accurately identify sensitive documents.

\section{Overview}
% TODO update overview with newly added sections:
%   - Literature Review - Explanations in Machine Learning

We start with a review of the literature on the application of TAR to sensitivity review, then explore some of the works on the use of TAR in the legal profession, notably in legal discovery and finally we survey some of the research into visualization of dense information spaces.
% TODO dense information spaces might a little bullshitty?
We then evaluate similar or otherwise related products, picking out key interesting features as well as antipatterns we wish to avoid.

In order to build the application, we delineate a scope, clarifying our end product, we also establish a set of requirements to prioritize features to implement.
We elaborate on the testing methodology for our user study and lastly we draw up wireframe diagrams to have a draft layout of components in the application and to get a better sense of feature priorities

We then discuss our application design process. We elaborate on our choice of technologies, the reasons for our final choice as well alternative options and why we did not choose them. We ultimately summarize our technology stack.

We detail some of the key pieces of software, highlight key sections of the code as well as our Repository and Continuous Integration setup.
We explain the issues we ran into, from data storage concerns to frontend implementation struggles as well as Machine Learning classifier optimization and debugging.

Lastly before concluding, we detail the findings from an evaluation by an expert panel, as well as the user evaluation we would have conducted had the COVID-19 pandemic not rendered it infeasible.


%==================================================================================================================================
\chapter{Background}

\section{Literature Review}

\subsection{Technology Assisted Sensitivity Review}

% TODO should I compare results between papers, can I talk about "improvements" over time? or not really since some results are not comparable (ex: focus on a particular FOIA exemption as opposed to all of S27 and S40)

% TODO elaborate on results
Research into the application of TAR to sensitivity review began with \textcite{de_rijke_towards_2014} which established an initial Machine Learning classifier combining the textual content of sensitive documents with sentiment analysis, a custom country risk score, entity and name recognition.

\autocite{sanchezDetectingSensitiveInformation2012}

% TODO be more specific about results compared to baseline
\textcite{mcdonaldUsingPartofSpeechNgrams2015} expanded on the idea and Improved classification with Part-of-Speech tagging of large N-grams to improve both recall and precision albeit on a more focused task of identifying a subset of S27 FOIA exemptions: ``information supplied in confidence''.

% TODO talk about F2 improvements with best SVM kernel (Gaussian) for POS standalone classification?
This work was continued with an in depth study of SVM kernel functions' performance for POS sequence classification \autocite{mcdonaldStudySVMKernel2017}.
The paper also combined the POS sequence classification with full text classification to obtain a 6.09\% improvement in the $F_{2}$ measure compared to the standalone baseline text classification.

\autocite{jose_enhancing_2017} Improving classification with word embeddings

\autocite{pasi_active_2018} Active Learning ("loopback learning") implemeting reviewer feedback in Sensitivity prediction

\autocite{mcdonaldHowSensitivityClassification2019} User study with Machine Learning classification

CITE SOME GENERAL ML EXPLANATIONS PAPERS

\subsection{``Predictive Coding'' or \textit{TAR} for legal electronic discovery}

Ongoing research in the area of Judicial practice has applied the concept of TAR to the process of legal discovery.
This research has coined the term ``Predictive Coding'' \autocite{carrollGrossmancormackGlossaryTechnologyassisted2013} as an industry specific use of TAR in the legal practice, particularly for ``legal discovery'', the pre-trial exchange of evidence by both parties.
Parallels can be drawn with the task of sensitivity review, specifically the process of identifying ``privileged information'' in a legal discovery corpus.
% TODO elaborate on privilege (both attorney-client and common interest)

\textcite{grossmanTechnologyAssistedReviewEDiscovery2010} compared the performances of a manual and TAR aided eDiscovery process with data from the various teams of the TREC 2009 Legal Track \autocite{hedinOverviewTREC2009}.
They conclude that TAR methods lead to more accurate eDiscovery relevance estimations with significantly lower effort than with an exhaustive manual discovery process.


One of these teams, \textcite{cormackMachineLearningInformation2009}, constructed various training sets for a given list of discovery topics from a corpus of eDiscovery documents using IR techniques (a search engine).
They then fitted Logistic regression classifiers on these training sets to classify the rest of the collection as relevant or not for each topic.
Ultimately, this team obtained the best F1 measure averaged across all topics \autocite{hedinOverviewTREC2009}.
Moreover, ``by all measures, the average efficiency and effectiveness of the five technology-assisted reviews surpasses that of the five manual reviews'' \autocite[p.~43]{grossmanTechnologyAssistedReviewEDiscovery2010}.

The task at hand is different from the sensitivity review of sensitive documents.
Firstly, the implications of missing a relevant document in sensitivity review can be far greater than in legal discovery.
The works we have cited above only manually reviewed fractions of their document collection, accepting a level of ``misclassification'' that is not tolerable in sensitivity review.
% TODO Source this in the works of the Macdonalds
Indeed, even with reliable classifiers, all documents will need to be manually reviewed for sensitivities.
The task of identifying ``privileged'' information in legal discovery documents is the one that comes closest that of sensitivity review.

\section{Explanations in Machine Learning}


\autocite{ribeiroWhyShouldTrust2016}

\autocite{lundbergUnifiedApproachInterpreting2017}

\section{Document visualization techniques}

\subsection{Lexical Episode Plots}
%
\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    \begin{subfigure}[c]{0.2\textwidth}
        \centering
        \small
        \includegraphics[width=\linewidth]{images/document_visualization/topic-overview.png}
        \caption{Lexical Episode Plots}
        \label{fig:lexical_plot}
    \end{subfigure}
    \hspace{0.01\textwidth}
    \begin{subfigure}[c]{0.17\textwidth}
        \centering
        \small
        \includegraphics[width=\linewidth]{images/related_products/vscode_minimap.png}
        \caption{VSCode Minimap}
        \label{fig:vscode_minimap}
    \end{subfigure}
    \caption{Document Overviews}
    \vspace{-40pt}
    \label{fig:test-modes}
\end{wrapfigure}

A variation of this kind of document overview displays highlights beside the document outline also showing the ``topic'' of the highlight (Figure \ref{fig:lexical_plot}), it is called ``Lexical Episode Plots'' \autocite{el-assadyVisArgueVisualText2016,goldExploratoryTextAnalysis2015}.

It is reminiscent of code overviews in text editors, especially those aimed at editing code, which provide an overview of the ``shape'' of the text with syntax highlighting colour to allow fast navigation between sections of source code like what \textcite{MicrosoftVscode2020} implements (Figure \ref{fig:vscode_minimap}).

An old Information Retrieval system, J24 \autocite[7]{ogdenDocumentThumbnailVisualizations1998}, implements a similar feature, highlighting query terms in a document outline
This shows that perhaps, these sort of overviews could also be used before viewing a document, as a popup view when a document is hovered on within a collection.

\subsection{Playing with font size}

\textcite{stoffelDocumentThumbnailsVariable2012} implement a combination of text highlighting along with font size variations for creating distorted thumbnails for previewing important document features.
It is not certain this would be a good thumbnail, the highlighted portions definitely show where key features are in the document, but a lot of the document structure is lost.
However varying font size within a \textit{reasonable range} ould be an interesting visualization for the visualizing feature importance in a prediction.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{images/document_visualization/font-size.png}
    \caption{Adjusting font size for overview of important features \\ \protect\autocite{stoffelDocumentThumbnailsVariable2012}}
    \label{fig:font-size}
\end{figure}


\subsection{Distortion}

This technique has many names and variants ``Document Lens'' \autocite{robertsonDocumentLens1993}, ``Fisheye'' view \autocite{greenbergFisheyeTextEditor1996} or ``hybrid continuous zoom'' \autocite{bartramContinuousZoomConstrained1995} with different implementations (see Figure \ref{fig:fisheyes}) they are part of a concept called ``Bifocal Display'' \autocite{apperleyBifocalDisplay}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/document_visualization/different-fisheyes.png}
    \caption{Different Fisheye views, from left to right: \\ Manhattan lens, zoomscapes, central perspective and parallel projection \\ \protect\autocite{baudischFishnetFisheyeWeb2004}}
    \label{fig:fisheyes}
\end{figure}

\textit{Circular Fisheye Distortion} is a circular ``magnifying glass'' effect visible in \textcite{bostockFisheyeGrid2019}. \textit{Cartesian Distortion} ``magnifies continuously so as to avoid local minification'' \autocite{bostockFisheyeDistortion2012}. The effect can also be limited to only one axis (vertical or horizontal) as seen in \textcite{pstuffaCartesianFisheyeDistortion2019}. In our case, Vertical Cartesian Distortion on a per line basis seems like an interesting visualization for document overview with emphasis on hovered lines. There is a D3js implementation of this effect (see the links above), it also seems like there is a React specific implementation of this text fisheye effect \autocite{zhongVincentdchanReactfisheye2019}.

\section{Related products}

\subsection{PDF editors}

\begin{wrapfigure}{r}{0.5\textwidth}
    \includegraphics[width=\linewidth]{images/related_products/adobe_redaction.png}
    \caption{Acrobat redaction workflow}
    \vspace{-20pt}
    \label{fig:adobe-redaction}
\end{wrapfigure}

Quite a few official government guidelines on the redaction of sensitive documents mention Adobe Acrobat as a go to tool for redacting text documents \autocite{thenationalarchivesRedactionToolkitPaper2016}.
Sometimes, the guidelines \textit{are} Adobe's guidelines for redaction \autocite{scottishgovernmentRedactingInformation2019}.

Adobe Acrobat is a well known PDF toolkit that notably enables sensitivity redaction of documents.
The principle is simple: select text, click redact and browse a nested context menu to select an exemption (Figure \ref{fig:adobe-redaction}).
This can be repetitive as their does not seem to be a way to redact multiple identical instances of the same piece of text at once.

There are a variety of PDF editors, when they implement document redaction, they closely resemble this workflow.
For instance, Foxit PhantomPDF is another well known PDF editor that implements a strikingly similar context menu (Appendix \ref{fig:foxit-menu}) also allows for document wide redaction of a text selection (Appendix \ref{fig:foxit-redact-all}), something which Adobe implement quite poorly (Appendix \ref{fig:adobe-redact-all}).

\subsection{Dedicated document redaction tools}

\begin{wrapfigure}{l}{0.4\textwidth}
    \includegraphics[width=\linewidth]{images/related_products/eredact_dropdown.png}
    \caption{e-Redact exemption select}
    \vspace{-17pt}
    \label{fig:eredact-dropdown}
\end{wrapfigure}

There are a few tools built specifically for redacting sensitive information,for example \textcite{ERedact} is an Office plugin that for redacting sensitive information from Word documents.
It's context menu is even more deeply nested than adobe Reader and contains exemptions from many legislations (Figure \ref{fig:eredact-dropdown}).
We want to avoid such a deep nesting of regularly used actions, for example in this case, it seems like a reviewer would rarely redact a document for US and UK FOIA at the same time.
e-Redact also adds the ability to collaborate with multiple people on document redactions, but beyond this, it is very similar to PDF Editors and their built-in functionalities.

e-Redact is also a purely manual tool, providing no ``aid'' to sensitivity redaction, contrarily to what we are trying to build.

\begin{wrapfigure}{r}{0.4\textwidth}
    \includegraphics[width=\linewidth]{images/related_products/redactedai_tooltip.png}
    \caption{Redacted.ai Tooltip}
    \label{fig:redactedai-tooltip}
\end{wrapfigure}

A lot of the PDF editors' redaction functionality also require one to traverse the screen to select a redaction.

Others such as \textcite{RedactedAIRemovea} use a tooltip above the text selection: the menu appears next to the selected text with actions to take on a selection which avoids having to cross long screen distances (Figure \ref{fig:redactedai-tooltip}).

In fact, \textcite{RedactedAIRemovea} is a similar concept to what we are trying to achieve. It is a web application for redacting sensitive documents using Natural Language Processing (NLP) methods (namely: entity extraction) to provide an aid to document reviewers.




%==================================================================================================================================
\chapter{Requirements}

\section{Scope}

\section{MoSCoW Functional Requirements}

% TODO reformulate this into something more formal
\begin{minipage}[t]{.5\linewidth}
    \centerline{\textbf{Must Have}}
    \begin{enumerate}[label=\textbf{M\arabic*}]
        \item document Set creation and deletion
        \item within a set, plaintext document creation and deletion
        \item for a document, binary sensitivity classification (sensitive or not?)
        \item explanation for aforementioned classifications
        \item user binary classification of a document (sensitive or not?)
        \item for a set of document, overall sensitivity statistics (number of sensitive documents etc…)
        \item for a set of documents, ordered of documents by sensitivity
        \item User redactions of a document with text highlighting, with possible edits and save functionality
        \item final redacted document exporting
        \item documentation for API
    \end{enumerate}
\end{minipage}
\hfill
\noindent
\begin{minipage}[t]{.5\linewidth}
    \centerline{\textbf{Should Have}}
    \begin{enumerate}[label=\textbf{S\arabic*}]
        \item user redactions helper (highlight all instances of redacted text, could do this on a document set level)
        \item for a document, suggest redaction of sensitive elements
        \item fulltext document search
    \end{enumerate}
\end{minipage}

\vspace{0.5cm}

\begin{minipage}[t]{.5\linewidth}
    \centerline{\textbf{Could Have}}
    \begin{enumerate}[label=\textbf{C\arabic*}]
        \item extract entities from document and display them (spaCy)
        \item possibility of using different text Classifiers (what for?)
        \item documentation for API SDK
        \item documentation for frontend
        \item reviewer authentication
    \end{enumerate}
\end{minipage}
\hfill
\noindent
\begin{minipage}[t]{.5\linewidth}
    \centerline{\textbf{Would Have}}
    \begin{enumerate}[label=\textbf{W\arabic*}]
        \item handle more than plain text files (PDF, Word etc…)
        \item specified and enforced document size and count upload limits
        \item ``Loopback Learning'' user redactions help improve future predictions
        \item deployed web application
    \end{enumerate}
\end{minipage}

\section{(non)Functional definition}

user study

\section{Wireframing}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=0.6\linewidth]{images/wireframes/doc_view.jpg}
        \caption{Document View Wireframe}
        \label{fig:document-wireframe}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \begin{subfigure}[b]{0.5\linewidth}
            \includegraphics[width=\linewidth]{images/wireframes/tooltip.jpg}
            \caption{minimized tooltip wireframe}
            \label{fig:tooltip-wireframe}
        \end{subfigure}
        \begin{subfigure}[b]{0.4\linewidth}
            \includegraphics[width=\linewidth]{images/wireframes/tooltip_comment.jpg}
            \caption{Expanded tooltip wireframe}
            \label{fig:expanded-tooltip-wireframe}
        \end{subfigure}
    \end{subfigure}
    \caption{Selection of some of the Wireframes drawn up during the design process}
    \label{fig:wireframes}

\end{figure}


%==================================================================================================================================
\chapter{Design}

link back to requirements

choice of technologies

\section{Machine Learning Model and explanations}

% TODO this sounds too much like a list

We used the Scikit-Learn library to implement our Machine Learning Models \autocite{pedregosaScikitlearnMachineLearning2011} for predicting the sensitiity of documents to meet requirement \textbf{M3}, \textbf{M7} and \textbf{M6}. For performance reasons, we replaced its SVC implementation with a drop-in parallel replacement ``ThunderSVM'' which can operate on multicore CPUs or CUDA enabled GPUs \autocite{wenThunderSVMFastSVM2018}.

We satisfy our requirement \textbf{M4} for explanations and more generally better understanding of our classifier's predictions using techniques implemented by the Lime library \autocite{ribeiroWhyShouldTrust2016} which provides per-feature weights explaining the feature's contribution to the final classification.

% TODO 
\autocite{lundbergUnifiedApproachInterpreting2017} Shapley values


\section{OpenAPI specification and generator}

``The OpenAPI Specification is a community-driven open specification within the OpenAPI Initiative, a Linux Foundation Collaborative Project.'' \autocite{OAIOpenAPISpecification2020}.
% TODO elaborate a bit more on this
It is an increasingly popular framework for formally defining a REST API contract.
Its precision and specificity is such that it has allowed powerful tools to expand it.
Notably, the OpenAPI generator \autocite{OpenAPIToolsOpenapigenerator2020} which, given an OpenAPI specification, allows for the \textit{automatic} generation of API server code in more than 40 languages and API client code in more than 50.
Hence, we chose to write our API specification in OpenAPI format \textit{before} developing the API.
This allowed us to generate boilerplate API server and client code with documentation simply given the precise documentation thus fulfilling requirements \textbf{C3} and \textbf{M10}.

\section{API Server}

Most of our data science tools are written in Python which is why we selected it to write our API server.
Furthermore, the language's broad library selection makes development easier, notably for implementing requirements such as \textbf{W1} and \textbf{C5}
The question of the choice of framework, remained, specifically since the OpenAPI generator allows for the generation of Python Server code with different frameworks.

We chose the implementation that makes use of an OpenAPI wrapper around Flask called Connexion which handles ``the mapping from the specification to the code.
This incentivizes you to write the specification'' \autocite{ZalandoConnexion2020}.
Furthermore, Flask is a Python API framework that is extensible with libraries notably implementing file upload size limits for requirement \textbf{W2}. Furthemore it is widely used by major technology companies like Reddit or Netflix \autocite{WhyDevelopersFlask}, a testament to its robustness and level of support which streamlines the implementation of requirement \textbf{M9}.

Furthermore, the Connexion+Flask implements the Oauth2 Authentication specification for API endpoints, a state of the art authentication framework that allows us to securely authenticate reviewers for requirements \textbf{C5} \autocite{jonesOAuthAuthorizationFramework2012}.

\section{Frontend}

We chose to implement a web application as the frontend for our application.
This allows for compatibility with many devices: generally anything that can run a reasonably modern web browser can use a web application without worrying about Operating System choice or GUI library compatibility.
We identified early on the need for a ``dynamic'' application meaning that in order to implement a web application, the codebase would be prominently Javascript (JS).

There are a number of modern frameworks that facilitate JS web application development and avoid having to write plain JS.
Their ``component'' approach to UI elements allows for the development of standalone modules that can be combined into an application, streamlining development by relying on high quality component libraries such as \textcite{MuiorgMaterialui2020} or visualization plotting libraries like \textcite{RechartsRecharts2020}.
There are three major web JS frameworks: \textcite{AngularAngular2020,FacebookReact2020,VuejsVue2020}.

Angular is a web framework developed by Google, development uses TypeScript (TS) which is a type language that can transcompile into Javascript.
It has a strict separation of styles (CSS), markup (HTML) and function (TS) and is generally much more ``opinionated'' on application structure and development.
These ``restrictions'' can, at the scale of large projects become standards enabling more streamlined collaboration, at our scale however (one developer), we viewed them as more of a hindrance than an enablement especially since our scope is that of a \textit{Proof of Concept} more than a final large scale project \autocite{wohlgethanSupportingWebDevelopmentDecisions2018}.

VueJS holds similar ``opinions'' on project structure but encapsulates all aspects of a component into a \lstinline{.vue} file albeit while being more ``liberal'' than Angular (by externalizing State management and routing for example) \autocite{wohlgethanSupportingWebDevelopmentDecisions2018}.
Furthermore, like Angular, VueJS introduces a special syntax for enhancing the templates with \lstinline{for} loops and \lstinline{if} statements.

Perhaps the ``most flexible'' framework is ReactJS which we chose as our frontend framework.
ReactJS uses \lstinline{.jsx} files which allow for the integration of HTML and CSS within JS, which avoids the custom syntax used by Angular and VueJS and allows for the versatile use of JS for introducing logic into templates.
Furthermore ReactJS is an Open Source project developed by Facebook, this backing along with its flexibility has given birth to a large ecosystem of both development support and advanced component libraries \autocite{wohlgethanSupportingWebDevelopmentDecisions2018}.
It does however introduce more complex state management due to its ``one-way data binding'': information flows from parent to child component but the reverse operation needs to be done through callbacks or using a state management libraries like \textcite{ReduxjsRedux2020}.
These problems mostly become prominent on larger scale projects, hence we opted for the flexibility of ReactJS for a quickly starting development, albeit encountering some of these state management issues in the latter stages of the project.
% TODO refer back to proof of concept scope of project to justify this

\section{Application Structure and Technology stack}

\begin{wrapfigure}{r}{0.5\textwidth}
    \includegraphics[width=\linewidth]{figures/tech_stack.png}
    \caption{Harpocrates tech stack}
    \label{fig:tech_stack}
\end{wrapfigure}

%==================================================================================================================================
\chapter{Implementation}


show coponennt code and how it gets data from backend

show lime explanations generation

key ``features'' code overview


\begin{itemize}
    \item Hands on SkLearn framework, experimented with different text pre-processing techniques and classifiers to build a Pipeline to classify textual documents into either sensitive or not sensitive categories
    \item overview of Machine Learning model explanation techniques, successfully implemented the Lime explainer, tried to use the SHAP explainer, but was unsuccessful
    \item Explore text document storage, looked at Terrier, ElasticSearch to finally settle on MongoDB
    \item Packaged SkLearn classifier into Python Flask API with OpenAPI specification
    \item created Javascript API Client from code generated from OpenAPI specification
    \item hands on ReactJS Javascript frontend framework, implemented a document browsing, viewing and upload frontend
    \item added text redaction feature to mark sections as sensitive with a label explaining why it is deemed sensitive
    \item implemented Lime Model explanations in Python backend, exposed them to the Flask API and created a frontend "in text" visualization of these sensitivities
    \item Research Document visualization techniques (literature review)
    \item created a populator script to load and classify all documents
    \item graph visualization of feature contribution to the classification and other UI tweaks
\end{itemize}

\section{Documents, labels and sampling}

We resample our highly unbalanced dataset with an extension to Scikit-Learn: ``imblearn'' which implements common resampling techniques\autocite{lemaitreImbalancedlearnPythonToolbox2017}.

%==================================================================================================================================
\chapter{Evaluation}

We conducted a user evaluation in order to evaluate the effectiveness of our application in helping reviewers conduct a sensitivity review. We sought to answer a handeful of research questions.

\begin{itemize}
    \item Firstly, does our visualization of predicted document sensitivity and explanation features help reviewers conduct a sensitivity review faster?
    \item Secondly, does it improve the accuracy of the sensitivity review process.
    \item Lastly, does it improve reviewers' confidence in the completeness of their sensitivity review
\end{itemize}

\section{Preparations and Experimental Setup}

Our collection is a set of 3801 Government documents relating to International Activities sensitivity reviewed by government sensitivity reviewers.
The ground truth for document sensitivity was established with respect to sections 40 (S40) (Personal Information) and 27 (S27) (International Relations) of the Freedom of Information Act (FOIA).

Due to our lack of access to expert document reviewers, we conducted our study thanks to students either from an academic politics background or with International Relations awareness.
We focused on identifying ``Personal Information'' exceptions to the FOIA which we deemed more ``attainable'' given our non-expert test subjects.

Our dataset contains 289 documents with S40 exemptions, we split the collection in order to obtain a stratified sample ``test set'' 20\% the size of the entire collection.
We vectorized the remaining 80\% ``train set'' using Scikit-Learn's \verb|TfidfVectorizer| \autocite{pedregosaScikitlearnMachineLearning2011}.
We resampled the resulting vectors to achieve better class balance with imblearn's \verb|SMOTEENN|, a combination of majority class downsampling and minority class upsampling \autocite{lemaitreImbalancedlearnPythonToolbox2017}.
We used the resulting vectorized and resampled documents to train a GPU accelerated SVC \autocite{wenThunderSVMFastSVM2018} for binary classification of documents depending on whether or not they contain sensitive information.

Due to time constraints, we sampled only sampled documents to review from the test set that were under 2000 characters.
We then selected both sensitive and non-sensitive documents in order to represent every category of the confusion matrix for the classifier with the following counts:

\begin{table}[H]
    \begin{tabular}{l ll}
                                & Actually non-sensitive & Actually Sensitive \\
        Predicted non-sensitive & 3                      & 1                  \\
        Predicted Sensitive     & 1                      & 1
    \end{tabular}
    \caption{Count of selected documents in the classifier's confusion matrix}
    \label{tab:confusion-matrix-selection}
\end{table}


We sampled test documents accordingly twice: once for each User Interface (UI) used in the evaluation.
One interface \textit{test mode 1} (Figure \ref{fig:test-mode-1}) is a simplified version of the final interface containing the classification prediction and the accompanying explanations.
The other, \textit{test mode 2} (Figure \ref{fig:test-mode-2}) is a stripped down version that only displays the document, and the manual redaction tools (document title and sensitivity type selection).
Our reviewers reviewed documents with both interfaces, the first interface displayed was selected randomly.


\begin{figure}[H]
    \centering
    \begin{subfigure}[c]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/ui_test_mode_1.png}
        \caption{Test mode 1}
        \label{fig:test-mode-1}
    \end{subfigure}
    \begin{subfigure}[c]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/ui_test_mode_2.png}
        \caption{Test mode 2}
        \label{fig:test-mode-2}
    \end{subfigure}
    \caption{The two test modes used for evaluation}
    \label{fig:test-modes}
\end{figure}

% TODO put questionnaire in appendix 
% TODO link questionnaire here
Lastly, after the review of a batch of documents with each interface, a questionnaire was handed out to the testers

\section{Evaluation}

As mentioned, each user will be asked to review one collection of 6 documents for personal information sensitivities (FOIA Section 27) per user interface

The independent variables will a set of two user interfaces: with and without the predicted classification and explanations (more details below) which all users will both use. We will measure multiple dependent variables:

\begin{itemize}
    \item Firstly, we will measure the time to review an entire collection with each interface.
    \item Secondly we will record the accuracy of each reviewer on each interface for all documents.
    \item Lastly, we will evaluate the confidence of the reviewers after using each interface with a Likert scale in the questionnaires.
\end{itemize}




Research questions
dependent/indepdent variables
UI variations
experimental setup
document, subject seleciton


results


link back to requirements
how have they been met
unit testing




%==================================================================================================================================
\chapter{Conclusion}


requirements I've met


reflections

future work

%==================================================================================================================================
%
% 
%==================================================================================================================================
%  APPENDICES  

\begin{appendices}
    \settocdepth{chapter}
    \chapter{PDF Editors}
    \section{PhantomPDF context menu}
    \label{fig:foxit-menu}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\linewidth]{images/related_products/foxit_redaction.png}
    \end{figure}
    \section{PhantomPDF redact all}
    \label{fig:foxit-redact-all}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\linewidth]{images/related_products/foxit_redact_all.png}
    \end{figure}
    \section{Adobe Acrobat redact all}
    \label{fig:adobe-redact-all}
    \begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{images/related_products/adobe_redact_all.png}
    \end{figure}
    \chapter{Wireframes}
    \section{Homepage}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\linewidth]{images/wireframes/home.jpg}
    \end{figure}
    \section{Set View}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.65\linewidth]{images/wireframes/set.jpg}
    \end{figure}
    \section{Set View - menu open}
    \begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{images/wireframes/set-menu.jpg}
    \end{figure}
    \section{Document View v1}
    \begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{images/wireframes/page.png}
    \end{figure}
    \section{Document View v2 - opened settings drawer}
    \begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{images/wireframes/doc_view.jpg}
    \end{figure}
    \section{Document View v2 - closed settings drawer}
    \begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{images/wireframes/doc-view.jpg}
    \end{figure}
    \section{Text Select Tooltip}
    \begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{images/wireframes/tooltip.jpg}
    \end{figure}
    \section{Text Select Tooltip - Opened Comment menu}
    \begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{images/wireframes/tooltip_comment.jpg}
    \end{figure}
\end{appendices}
%==================================================================================================================================
%   BIBLIOGRAPHY   


\newpage


\section*{References}

\printbibliography[heading=none]

\vspace*{\fill}
% print license
\doclicenseThis

\end{document}
